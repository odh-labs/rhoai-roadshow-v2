{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0bfa248-2241-4cb8-956a-d2e00afd02e4",
   "metadata": {},
   "source": [
    "# Level 3: Deploy a new GPU workload\n",
    "\n",
    "Now we have a new GPU node, lets safely isolate a new workload using it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2953cc2",
   "metadata": {},
   "source": [
    "We talked about <a href=\"https://github.com/odh-labs/rhoai-roadshow/blob/main/site/docs/6-gpuaas/notebooks/Level1_add_gpu_node.ipynb\" target=\"_blank\">GPU Concurrency</a> earlier on, now we want to create a new workload with our new GPU Node.\n",
    "\n",
    "If you recall, the new AWS instance type had the following capacity:\n",
    "\n",
    "|Instance Name | vCPUs | Memory (GiB) | NVIDIA A10 GPU | GPU Memory (GiB) | Network Bandwidth (Gbps) | EBS Bandwidth (Gbps) |\n",
    "|--------------|-------|--------------|----------------|------------------|--------------------------|----------------------|\n",
    "|  g5.xlarge   |   4   |      16      |\t      1        |\t      24\t     |           10\t            |          3.5         |\n",
    "\n",
    "So, we are quite limited in terms of vCPU, RAM and GPU.\n",
    "\n",
    "We want to configure out new GPU node for our marketing department. \n",
    "\n",
    "They want to do image generation using a model.\n",
    "\n",
    "We have <a href=\"https://github.com/odh-labs/rhoai-roadshow/blob/main/site/docs/6-gpuaas/notebooks/Level2_gpu_operator.ipynb\" target=\"_blank\">configured our GPU</a> for use, however given the limited resources what else can we do to ensure that only our marketing department's applications make user of this resource? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a3a7cc",
   "metadata": {},
   "source": [
    "## Workload Scheduling\n",
    "\n",
    "There are many levels of isolation within OpenShift. Common patterns for separating tenant workloads include:\n",
    "\n",
    "- Give each tenant their own OpenShift cluster (this has become a lot easier with [OpenShift Hosted Control Planes](https://www.redhat.com/en/topics/containers/what-are-hosted-control-planes))\n",
    "- Use OpenShift's projects and namespaces - this leverages a cluster's Role Based Access Control (RBAC)\n",
    "\n",
    "We also need to ensure that workloads land on the nodes we want. \n",
    "\n",
    "To do this we are going to make use of node selectors, taints and tolerations and network policy.\n",
    "\n",
    "We will explain what these are and how to use them as we go through this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ff24e5",
   "metadata": {},
   "source": [
    "First login to OpenShift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63b3037-a5ce-4a8e-a032-bca971dc5af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Using insecure TLS client config. Setting this option is not supported!\n",
      "\n",
      "Login successful.\n",
      "\n",
      "You have access to 106 projects, the list has been suppressed. You can list all projects with 'oc projects'\n",
      "\n",
      "Using project \"ai-roadshow\".\n",
      "Welcome! See 'oc help' to get started.\n"
     ]
    }
   ],
   "source": [
    "!oc login -u admin -p ${ADMIN_PASSWORD} --server=https://api.${BASE_DOMAIN}:6443 --insecure-skip-tls-verify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230b0126",
   "metadata": {},
   "source": [
    "We should see that both our SNO node, and extra GPU node are running OK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d43d581b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAMESPACE               NAME                                    PHASE     TYPE         REGION      ZONE         AGE\n",
      "openshift-machine-api   sno-5dqmr-master-0                      Running   g6.8xlarge   us-east-2   us-east-2a   4d19h\n",
      "openshift-machine-api   sno-5dqmr-worker-us-east-2a-gpu-kcx4p   Running   g5.xlarge    us-east-2   us-east-2a   32m\n"
     ]
    }
   ],
   "source": [
    "!oc get machines.machine.openshift.io -A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7f77c8",
   "metadata": {},
   "source": [
    "To talk about Taints and Tolerations in OpenShift/Kubernetes - we first have to have a basic understanding on the term `node affinity`.\n",
    "\n",
    "Node affinity attracts pods to a specific set of nodes.\n",
    "\n",
    "This can work as a hard requirement or merely a scheduling preference.\n",
    "\n",
    "A **taint** works in the opposite way to this - it is used to repel a given set of pods from a node.\n",
    "\n",
    "You can apply one or more taints to a node. \n",
    "\n",
    "That way, you mark that the node shouldn窶冲 accept any pods that happen not to **tolerate** these taints.\n",
    "\n",
    "So **tolerations** then are applied to pods and let the Kubernetes scheduler schedule pods on nodes with matching taints.\n",
    "\n",
    "A toleration allows scheduling but doesn窶冲 _guarantee_ it. That窶冱 because the scheduler takes into account other parameters as well.\n",
    "\n",
    "So, let's check out our nodes for any **taints** they may have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4150386c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ip-10-0-15-75.us-east-2.compute.internal    <none>\n",
      "ip-10-0-29-181.us-east-2.compute.internal   <none>\n"
     ]
    }
   ],
   "source": [
    "!oc get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints --no-headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82aac1b",
   "metadata": {},
   "source": [
    "OK, so currently the nodes have **none** - no taints. A node can have one or more taints.\n",
    "\n",
    "Now we want to user taints to repel pods from the g5.xlarge A10 GPU node.\n",
    "\n",
    "Taints have a key, value, and effect.\n",
    "\n",
    "- The effect determines how the taint affects pod scheduling:\n",
    "  - `NoSchedule`: The Kubernetes scheduler will only allow pods with a matching toleration to be scheduled on the node. \n",
    "  - `PreferNoSchedule`: The scheduler will try to avoid scheduling pods without a matching toleration, but it's not a hard requirement. \n",
    "  - `NoExecute`: Pods without a matching toleration will be evicted from the node.\n",
    "\n",
    "Lets create a taint for out A10 GPU node. We will use the `key: gpu` `value: NVIDIA-A10G-SHARED` and effect of `PreferNoSchedule` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "500f12fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node/ip-10-0-15-75.us-east-2.compute.internal tainted\n"
     ]
    }
   ],
   "source": [
    "!oc adm taint nodes -l nvidia.com/gpu.product=NVIDIA-A10G-SHARED gpu=NVIDIA-A10G-SHARED:PreferNoSchedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d61739",
   "metadata": {},
   "source": [
    "Great, let's check the nodes again for **taints**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b9ca4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ip-10-0-15-75.us-east-2.compute.internal    [map[effect:PreferNoSchedule key:gpu value:NVIDIA-A10G-SHARED]]\n",
      "ip-10-0-29-181.us-east-2.compute.internal   <none>\n"
     ]
    }
   ],
   "source": [
    "!oc get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints --no-headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8722b790",
   "metadata": {},
   "source": [
    "Nice, our node taint is there.\n",
    "\n",
    "In the <a href=\"https://github.com/odh-labs/rhoai-roadshow/blob/main/site/docs/6-gpuaas/notebooks/Level2_gpu_operator.ipynb\" target=\"_blank\">previous notebook</a> we configured a `HardwareProfile` for out GPU, let's update that with a matching **toleration** i.e. any notebook that uses this hardware profile will tolerate the node taint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f94a169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardwareprofile.dashboard.opendatahub.io/nvidia-a10-shared configured\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "oc apply -f- << EOF\n",
    "apiVersion: dashboard.opendatahub.io/v1alpha1\n",
    "kind: HardwareProfile\n",
    "metadata:\n",
    "  annotations:\n",
    "    opendatahub.io/dashboard-feature-visibility: '[]'\n",
    "  name: nvidia-a10-shared\n",
    "  namespace: redhat-ods-applications\n",
    "spec:\n",
    "  description: \"\"\n",
    "  displayName: Nvidia A10 (Shared)\n",
    "  enabled: true\n",
    "  identifiers:\n",
    "  - defaultCount: 2\n",
    "    displayName: CPU\n",
    "    identifier: cpu\n",
    "    maxCount: 4\n",
    "    minCount: 1\n",
    "    resourceType: CPU\n",
    "  - defaultCount: 4Gi\n",
    "    displayName: Memory\n",
    "    identifier: memory\n",
    "    maxCount: 8Gi\n",
    "    minCount: 2Gi\n",
    "    resourceType: Memory\n",
    "  - defaultCount: 1\n",
    "    displayName: nvidia.com/gpu\n",
    "    identifier: nvidia.com/gpu\n",
    "    minCount: 1\n",
    "    resourceType: Accelerator\n",
    "  nodeSelector:\n",
    "    nvidia.com/gpu.product: NVIDIA-A10G-SHARED\n",
    "  tolerations:\n",
    "    - effect: PreferNoSchedule\n",
    "      operator: Equal\n",
    "      key: gpu\n",
    "      value: NVIDIA-A10G-SHARED\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0782029",
   "metadata": {},
   "source": [
    "Now we can create some workloads that make use of these node affinity settings.\n",
    "\n",
    "It is common to use taints and tolerations along with a `nodeSelector` (which attracts pods to nodes). \n",
    "\n",
    "This way we can be sure that the workload lands on the A10 GPU node whilst pods without the toleration will be repelled.\n",
    "\n",
    "We can run a single pod that has CUDA libraries loaded for NVIDIA (this is a vLLM serving pod we are using for one of our LLMs already).\n",
    "\n",
    "We specify the command to be `sleep inf` i.e. the pod waits forever doing nothing, as well as setting resource limits and our node affinity settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d964586f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/tools created\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "oc -n ai-roadshow run tools --image=quay.io/eformat/vllm:latest-bnb --overrides='\n",
    "{\n",
    "\"apiVersion\": \"v1\",\n",
    "\"kind\": \"Pod\",\n",
    "\"spec\": {\n",
    "  \"containers\": [\n",
    "    {\n",
    "      \"name\": \"kserve-container\",\n",
    "      \"image\": \"quay.io/eformat/vllm:latest-bnb\",\n",
    "      \"command\": [\"/bin/bash\", \"-c\", \"sleep inf\"],\n",
    "      \"resources\": {\n",
    "        \"limits\": {\n",
    "          \"nvidia.com/gpu\": \"1\"\n",
    "        },\n",
    "        \"requests\": {\n",
    "          \"nvidia.com/gpu\": \"1\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"nodeSelector\": {\n",
    "    \"nvidia.com/gpu.present\": \"true\",\n",
    "    \"nvidia.com/gpu.product\": \"NVIDIA-A10G-SHARED\"\n",
    "  },\n",
    "  \"tolerations\": [\n",
    "    {\n",
    "      \"effect\": \"PreferNoSchedule\",\n",
    "      \"operator\": \"Equal\",\n",
    "      \"key\": \"gpu\",\n",
    "      \"value\": \"NVIDIA-A10G-SHARED\"\n",
    "    }\n",
    "  ]\n",
    "}}\n",
    "'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c120b3b",
   "metadata": {},
   "source": [
    "Let's check the pod ran OK and is scheduled on the right node with the A10 GPU. It may take a minute or two to pull the image to the Node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aa0955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME    READY   STATUS    RESTARTS   AGE     IP            NODE                                       NOMINATED NODE   READINESS GATES\n",
      "tools   1/1     Running   0          7m33s   10.129.0.27   ip-10-0-15-75.us-east-2.compute.internal   <none>           <none>\n"
     ]
    }
   ],
   "source": [
    "!oc -n ai-roadshow get pods tools -o wide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489aa82b",
   "metadata": {},
   "source": [
    "Great. We can do ther check, we could `oc rsh tools` into the pod (or use the `Terminal` in the OpenShift console) to run `nvtop`\n",
    "or we can check the output of the `nvidia-smi` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19151e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun 29 23:25:21 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A10G                    On  |   00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   35C    P8             24W /  300W |       0MiB /  23028MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!oc -n ai-roadshow exec tools -- nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8163ff10",
   "metadata": {},
   "source": [
    "We see the `NVIDIA A10G` listed - so we know the pod can see the GPU OK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b5548f",
   "metadata": {},
   "source": [
    "## Modify the workbench\n",
    "\n",
    "We are going to modify the workbench so it runs on our new GPU node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ce13dd",
   "metadata": {},
   "source": [
    "From the OpenShift web console - stop (or recreate) the `gpuaas` notebook, and assign the `Nvidia A10 (Shared)` Hardware Profile. \n",
    "\n",
    "![images/gpuaas-a10-workbench.png](images/gpuaas-a10-workbench.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3c9be1",
   "metadata": {},
   "source": [
    "Now (re)start the workbench (i know, you are probably reading this in that workbench !!)\n",
    "\n",
    "Because we correctly set up node affinity for our HardwareProfile, you should see the workbench pod correctly assigned to your A10 gpu node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a938298e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME       READY   STATUS    RESTARTS   AGE     IP            NODE                                       NOMINATED NODE   READINESS GATES\n",
      "gpuaas-0   2/2     Running   0          4m29s   10.129.0.28   ip-10-0-15-75.us-east-2.compute.internal   <none>           <none>\n"
     ]
    }
   ],
   "source": [
    "!oc get pods gpuaas-0 -o wide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360808fc",
   "metadata": {},
   "source": [
    "## Let's have some fun generating Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e6b599",
   "metadata": {},
   "source": [
    "Our marketing department are creative cats 汾 and love to use AI to generate images for their marketing campaigns.\n",
    "\n",
    "The want to use the open source [ComfyUI](https://www.comfy.org/) tool. So lets try it out now in our notebook.\n",
    "\n",
    "Clone the following repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdc14f9f-7b3f-4b90-a210-1f9b00f267fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ComfyUI'...\n",
      "remote: Enumerating objects: 20719, done.\u001b[K\n",
      "remote: Total 20719 (delta 0), reused 0 (delta 0), pack-reused 20719 (from 1)\u001b[K\n",
      "Receiving objects: 100% (20719/20719), 70.86 MiB | 31.56 MiB/s, done.\n",
      "Resolving deltas: 100% (13815/13815), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/comfyanonymous/ComfyUI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a0d770",
   "metadata": {},
   "source": [
    "Change directories to `ComfyUI`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8c07396-20db-4aed-9454-5c9784c2b059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/app-root/src/rhoai-roadshow/site/docs/6-gpuaas/notebooks/ComfyUI\n"
     ]
    }
   ],
   "source": [
    "%cd ComfyUI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd64fbc0",
   "metadata": {},
   "source": [
    "Now install the python dependencies CopfyUI needs to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781ff6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab9c645-2421-4e2d-a7c9-747735cfce2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xformers!=0.0.18\n",
      "  Downloading xformers-0.0.31-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting comfyui-frontend-package==1.23.4 (from -r requirements.txt (line 1))\n",
      "  Downloading comfyui_frontend_package-1.23.4-py3-none-any.whl.metadata (117 bytes)\n",
      "Collecting comfyui-workflow-templates==0.1.30 (from -r requirements.txt (line 2))\n",
      "  Downloading comfyui_workflow_templates-0.1.30-py3-none-any.whl.metadata (55 kB)\n",
      "Collecting comfyui-embedded-docs==0.2.3 (from -r requirements.txt (line 3))\n",
      "  Downloading comfyui_embedded_docs-0.2.3-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting torch (from -r requirements.txt (line 4))\n",
      "  Downloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting torchsde (from -r requirements.txt (line 5))\n",
      "  Downloading torchsde-0.2.6-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting torchvision (from -r requirements.txt (line 6))\n",
      "  Downloading torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio (from -r requirements.txt (line 7))\n",
      "  Downloading torchaudio-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting numpy>=1.25.0 (from -r requirements.txt (line 8))\n",
      "  Downloading numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting einops (from -r requirements.txt (line 9))\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting transformers>=4.37.2 (from -r requirements.txt (line 10))\n",
      "  Downloading transformers-4.53.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting tokenizers>=0.13.3 (from -r requirements.txt (line 11))\n",
      "  Downloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting sentencepiece (from -r requirements.txt (line 12))\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting safetensors>=0.4.2 (from -r requirements.txt (line 13))\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: aiohttp>=3.11.8 in /opt/app-root/lib64/python3.11/site-packages (from -r requirements.txt (line 14)) (3.11.18)\n",
      "Requirement already satisfied: yarl>=1.18.0 in /opt/app-root/lib64/python3.11/site-packages (from -r requirements.txt (line 15)) (1.20.0)\n",
      "Requirement already satisfied: pyyaml in /opt/app-root/lib64/python3.11/site-packages (from -r requirements.txt (line 16)) (6.0.2)\n",
      "Collecting Pillow (from -r requirements.txt (line 17))\n",
      "  Downloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Collecting scipy (from -r requirements.txt (line 18))\n",
      "  Downloading scipy-1.16.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting tqdm (from -r requirements.txt (line 19))\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: psutil in /opt/app-root/lib64/python3.11/site-packages (from -r requirements.txt (line 20)) (7.0.0)\n",
      "Collecting alembic (from -r requirements.txt (line 21))\n",
      "  Downloading alembic-1.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting SQLAlchemy (from -r requirements.txt (line 22))\n",
      "  Downloading sqlalchemy-2.0.41-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting kornia>=0.7.1 (from -r requirements.txt (line 25))\n",
      "  Downloading kornia-0.8.1-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting spandrel (from -r requirements.txt (line 26))\n",
      "  Downloading spandrel-0.4.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting soundfile (from -r requirements.txt (line 27))\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\n",
      "Collecting av>=14.2.0 (from -r requirements.txt (line 28))\n",
      "  Downloading av-14.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
      "Collecting pydantic~=2.0 (from -r requirements.txt (line 29))\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting pydantic-settings~=2.0 (from -r requirements.txt (line 30))\n",
      "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting filelock (from torch->-r requirements.txt (line 4))\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/app-root/lib64/python3.11/site-packages (from torch->-r requirements.txt (line 4)) (4.13.2)\n",
      "Collecting sympy>=1.13.3 (from torch->-r requirements.txt (line 4))\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch->-r requirements.txt (line 4))\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/app-root/lib64/python3.11/site-packages (from torch->-r requirements.txt (line 4)) (3.1.6)\n",
      "Collecting fsspec (from torch->-r requirements.txt (line 4))\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch->-r requirements.txt (line 4))\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch->-r requirements.txt (line 4))\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch->-r requirements.txt (line 4))\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch->-r requirements.txt (line 4))\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch->-r requirements.txt (line 4))\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch->-r requirements.txt (line 4))\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch->-r requirements.txt (line 4))\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch->-r requirements.txt (line 4))\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch->-r requirements.txt (line 4))\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch->-r requirements.txt (line 4))\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch->-r requirements.txt (line 4))\n",
      "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch->-r requirements.txt (line 4))\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch->-r requirements.txt (line 4))\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch->-r requirements.txt (line 4))\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.1 (from torch->-r requirements.txt (line 4))\n",
      "  Downloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/app-root/lib64/python3.11/site-packages (from triton==3.3.1->torch->-r requirements.txt (line 4)) (75.8.2)\n",
      "Collecting trampoline>=0.1.2 (from torchsde->-r requirements.txt (line 5))\n",
      "  Downloading trampoline-0.1.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers>=4.37.2->-r requirements.txt (line 10))\n",
      "  Downloading huggingface_hub-0.33.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/lib64/python3.11/site-packages (from transformers>=4.37.2->-r requirements.txt (line 10)) (25.0)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.37.2->-r requirements.txt (line 10))\n",
      "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib64/python3.11/site-packages (from transformers>=4.37.2->-r requirements.txt (line 10)) (2.32.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp>=3.11.8->-r requirements.txt (line 14)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp>=3.11.8->-r requirements.txt (line 14)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp>=3.11.8->-r requirements.txt (line 14)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp>=3.11.8->-r requirements.txt (line 14)) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp>=3.11.8->-r requirements.txt (line 14)) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp>=3.11.8->-r requirements.txt (line 14)) (0.3.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/app-root/lib64/python3.11/site-packages (from yarl>=1.18.0->-r requirements.txt (line 15)) (3.10)\n",
      "Collecting Mako (from alembic->-r requirements.txt (line 21))\n",
      "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting greenlet>=1 (from SQLAlchemy->-r requirements.txt (line 22))\n",
      "  Downloading greenlet-3.2.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting kornia_rs>=0.1.9 (from kornia>=0.7.1->-r requirements.txt (line 25))\n",
      "  Downloading kornia_rs-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/app-root/lib64/python3.11/site-packages (from soundfile->-r requirements.txt (line 27)) (1.17.1)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic~=2.0->-r requirements.txt (line 29))\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic~=2.0->-r requirements.txt (line 29))\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic~=2.0->-r requirements.txt (line 29))\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings~=2.0->-r requirements.txt (line 30))\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: pycparser in /opt/app-root/lib64/python3.11/site-packages (from cffi>=1.0->soundfile->-r requirements.txt (line 27)) (2.22)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub<1.0,>=0.30.0->transformers>=4.37.2->-r requirements.txt (line 10))\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch->-r requirements.txt (line 4))\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/app-root/lib64/python3.11/site-packages (from jinja2->torch->-r requirements.txt (line 4)) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib64/python3.11/site-packages (from requests->transformers>=4.37.2->-r requirements.txt (line 10)) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib64/python3.11/site-packages (from requests->transformers>=4.37.2->-r requirements.txt (line 10)) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib64/python3.11/site-packages (from requests->transformers>=4.37.2->-r requirements.txt (line 10)) (2025.4.26)\n",
      "Downloading comfyui_frontend_package-1.23.4-py3-none-any.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m182.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading comfyui_workflow_templates-0.1.30-py3-none-any.whl (71.4 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m71.4/71.4 MB\u001b[0m \u001b[31m410.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading comfyui_embedded_docs-0.2.3-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m594.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xformers-0.0.31-cp39-abi3-manylinux_2_28_x86_64.whl (117.1 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m117.1/117.1 MB\u001b[0m \u001b[31m196.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (821.2 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m338.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m169.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m170.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m167.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m471.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m131.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m132.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m380.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m140.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m133.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m132.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m135.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m135.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m144.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m138.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchsde-0.2.6-py3-none-any.whl (61 kB)\n",
      "Downloading torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl (7.5 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading torchaudio-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m242.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl (16.9 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m234.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading transformers-4.53.0-py3-none-any.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m238.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m240.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m476.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m256.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.16.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.3 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m236.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading alembic-1.16.2-py3-none-any.whl (242 kB)\n",
      "Downloading sqlalchemy-2.0.41-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m241.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kornia-0.8.1-py2.py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m529.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading spandrel-0.4.1-py3-none-any.whl (305 kB)\n",
      "Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m412.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading av-14.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.3 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m238.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m355.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading greenlet-3.2.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (585 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m585.5/585.5 kB\u001b[0m \u001b[31m559.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.33.1-py3-none-any.whl (515 kB)\n",
      "Downloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Downloading kornia_rs-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m232.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m418.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m245.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trampoline-0.1.2-py3-none-any.whl (5.2 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m395.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m239.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m546.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: trampoline, sentencepiece, nvidia-cusparselt-cu12, mpmath, typing-inspection, triton, tqdm, sympy, safetensors, regex, python-dotenv, pydantic-core, Pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, Mako, kornia_rs, hf-xet, greenlet, fsspec, filelock, einops, comfyui-workflow-templates, comfyui-frontend-package, comfyui-embedded-docs, av, annotated-types, SQLAlchemy, soundfile, scipy, pydantic, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, huggingface-hub, tokenizers, pydantic-settings, nvidia-cusolver-cu12, alembic, transformers, torch, xformers, torchvision, torchsde, torchaudio, kornia, spandrel\n",
      "Successfully installed Mako-1.3.10 Pillow-11.2.1 SQLAlchemy-2.0.41 alembic-1.16.2 annotated-types-0.7.0 av-14.4.0 comfyui-embedded-docs-0.2.3 comfyui-frontend-package-1.23.4 comfyui-workflow-templates-0.1.30 einops-0.8.1 filelock-3.18.0 fsspec-2025.5.1 greenlet-3.2.3 hf-xet-1.1.5 huggingface-hub-0.33.1 kornia-0.8.1 kornia_rs-0.1.9 mpmath-1.3.0 networkx-3.5 numpy-2.3.1 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 pydantic-2.11.7 pydantic-core-2.33.2 pydantic-settings-2.10.1 python-dotenv-1.1.1 regex-2024.11.6 safetensors-0.5.3 scipy-1.16.0 sentencepiece-0.2.0 soundfile-0.13.1 spandrel-0.4.1 sympy-1.14.0 tokenizers-0.21.2 torch-2.7.1 torchaudio-2.7.1 torchsde-0.2.6 torchvision-0.22.1 tqdm-4.67.1 trampoline-0.1.2 transformers-4.53.0 triton-3.3.1 typing-inspection-0.4.1 xformers-0.0.31\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install xformers!=0.0.18 -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeeade4",
   "metadata": {},
   "source": [
    "We need to grab a diffusion model that can generate images. There are a lot of them out there.\n",
    "\n",
    "A nice list of images can be seen in the [collab workbook](https://github.com/comfyanonymous/ComfyUI/blob/master/notebooks/comfyui_colab.ipynb) that is part of the ComfyUI repo.\n",
    "\n",
    "We will use the basic Stable Diffusion model from [**stabilityai**](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) which has a permissive license for usage.\n",
    "\n",
    "Let's download it locally (its about 6.5G in size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7b6c174-df2a-4af2-bd84-fe412df17d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-06-29 23:35:40--  https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors\n",
      "Resolving huggingface.co (huggingface.co)... 3.160.5.25, 3.160.5.76, 3.160.5.109, ...\n",
      "Connecting to huggingface.co (huggingface.co)|3.160.5.25|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/64bfcd5ff462a99a04fd1ec8/3d6f740fa52572e1071b8ecb7c5f8a8e2cbef596a51121102877bd9900078891?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250629%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250629T233540Z&X-Amz-Expires=3600&X-Amz-Signature=4f40ffaa8fb62381bad996a9328fb9ab92f8f961bc73815d9d3082d59187c340&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27sd_xl_base_1.0.safetensors%3B+filename%3D%22sd_xl_base_1.0.safetensors%22%3B&x-id=GetObject&Expires=1751243740&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MTI0Mzc0MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NGJmY2Q1ZmY0NjJhOTlhMDRmZDFlYzgvM2Q2Zjc0MGZhNTI1NzJlMTA3MWI4ZWNiN2M1ZjhhOGUyY2JlZjU5NmE1MTEyMTEwMjg3N2JkOTkwMDA3ODg5MSoifV19&Signature=KXyPs1uDD6SGLDxRGs06ktK0t7DRiQIS2fLBEaeU-YHOA2kC9LFuEbqyeKLzIvmFbAjKRp4egEZLuUjZOSPtOEAsT1VzpLLLjsKUgz6oMO8SdKZIBChFV%7ElBouh38qvqhrn-tfC2f-mMH-22XZiLwEmoG3AcVkwBD8ipt6z0Ug3D9di5ZXsrEJ%7E8jLmWsKTZ-Ndz1QGxR%7EdMe2BGWvq4-KaXcAezWUgS0tGV2xlXQBVGyrUQdsbXp-qkpdmJwNGVpaVaqzYblF%7E02W6GCZjTZGZ0pYoxlPlttNh%7EUi4gORBe2tDuKBMirUv5zGaEgvfTwa39dq-E2D3KEPQlFx5C5g__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
      "--2025-06-29 23:35:40--  https://cas-bridge.xethub.hf.co/xet-bridge-us/64bfcd5ff462a99a04fd1ec8/3d6f740fa52572e1071b8ecb7c5f8a8e2cbef596a51121102877bd9900078891?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250629%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250629T233540Z&X-Amz-Expires=3600&X-Amz-Signature=4f40ffaa8fb62381bad996a9328fb9ab92f8f961bc73815d9d3082d59187c340&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27sd_xl_base_1.0.safetensors%3B+filename%3D%22sd_xl_base_1.0.safetensors%22%3B&x-id=GetObject&Expires=1751243740&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MTI0Mzc0MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NGJmY2Q1ZmY0NjJhOTlhMDRmZDFlYzgvM2Q2Zjc0MGZhNTI1NzJlMTA3MWI4ZWNiN2M1ZjhhOGUyY2JlZjU5NmE1MTEyMTEwMjg3N2JkOTkwMDA3ODg5MSoifV19&Signature=KXyPs1uDD6SGLDxRGs06ktK0t7DRiQIS2fLBEaeU-YHOA2kC9LFuEbqyeKLzIvmFbAjKRp4egEZLuUjZOSPtOEAsT1VzpLLLjsKUgz6oMO8SdKZIBChFV%7ElBouh38qvqhrn-tfC2f-mMH-22XZiLwEmoG3AcVkwBD8ipt6z0Ug3D9di5ZXsrEJ%7E8jLmWsKTZ-Ndz1QGxR%7EdMe2BGWvq4-KaXcAezWUgS0tGV2xlXQBVGyrUQdsbXp-qkpdmJwNGVpaVaqzYblF%7E02W6GCZjTZGZ0pYoxlPlttNh%7EUi4gORBe2tDuKBMirUv5zGaEgvfTwa39dq-E2D3KEPQlFx5C5g__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
      "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 3.160.5.60, 3.160.5.77, 3.160.5.46, ...\n",
      "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|3.160.5.60|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6938078334 (6.5G)\n",
      "Saving to: 窶./models/checkpoints/sd_xl_base_1.0.safetensors窶兔n",
      "\n",
      "sd_xl_base_1.0.safe 100%[===================>]   6.46G   125MB/s    in 48s     \n",
      "\n",
      "2025-06-29 23:36:28 (138 MB/s) - 窶./models/checkpoints/sd_xl_base_1.0.safetensors窶 saved [6938078334/6938078334]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -c https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors -P ./models/checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28b6bfa",
   "metadata": {},
   "source": [
    "Before we run ComfyUI we need to install the `localtunnel` nodejs package so we can connect to the user interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8289c91-c9d0-4854-8340-e7b427711cf2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1G\u001b[0K笄兔u001b[1G\u001b[0K笄ｹ\u001b[1G\u001b[0K笄ｸ\u001b[1G\u001b[0K笄ｼ\u001b[1G\u001b[0K笄ｴ\u001b[1G\u001b[0K笄ｦ\u001b[1G\u001b[0K笄ｧ\u001b[1G\u001b[0K笄\u001b[1G\u001b[0K笄十u001b[1G\u001b[0K笄欺u001b[1G\u001b[0K笄兔u001b[1G\u001b[0K笄ｹ\u001b[1G\u001b[0K\n",
      "added 22 packages in 2s\n",
      "\u001b[1G\u001b[0K笄ｹ\u001b[1G\u001b[0K\n",
      "\u001b[1G\u001b[0K笄ｹ\u001b[1G\u001b[0K3 packages are looking for funding\n",
      "\u001b[1G\u001b[0K笄ｹ\u001b[1G\u001b[0K  run `npm fund` for details\n",
      "\u001b[1G\u001b[0K笄ｹ\u001b[1G\u001b[0K"
     ]
    }
   ],
   "source": [
    "!npm install localtunnel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ed29b0",
   "metadata": {},
   "source": [
    "OK, we are nearly there. We will create a Kubernetes `Service` that targets the port from this workbench we will run ComfyUI on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffae809-9185-4f30-8a8a-c36871d73fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service/comfy unchanged\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6 (iframe_thread):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/app-root/lib64/python3.11/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/usr/lib64/python3.11/threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_129/3747656451.py\", line 5, in iframe_thread\n",
      "NameError: name 'time' is not defined\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "oc apply -f- << EOF\n",
    "kind: Service\n",
    "apiVersion: v1\n",
    "metadata:\n",
    "  name: comfy\n",
    "  namespace: ai-roadshow\n",
    "spec:\n",
    "  ipFamilies:\n",
    "    - IPv4\n",
    "  ports:\n",
    "    - protocol: TCP\n",
    "      port: 8188\n",
    "      targetPort: 8188\n",
    "  internalTrafficPolicy: Cluster\n",
    "  type: ClusterIP\n",
    "  ipFamilyPolicy: SingleStack\n",
    "  sessionAffinity: None\n",
    "  selector:\n",
    "    statefulset: gpuaas\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548b7b89",
   "metadata": {},
   "source": [
    "And in OpenShift, we can expose this Service externally using a `Route`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8078f59-d3b6-4d87-b6a0-e3c99502b263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "route.route.openshift.io/comfy unchanged\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "oc apply -f- << EOF\n",
    "kind: Route\n",
    "apiVersion: route.openshift.io/v1\n",
    "metadata:\n",
    "  name: comfy\n",
    "  namespace: ai-roadshow\n",
    "spec:\n",
    "  to:\n",
    "    kind: Service\n",
    "    name: comfy\n",
    "    weight: 100\n",
    "  port:\n",
    "    targetPort: 8188\n",
    "  tls:\n",
    "    termination: edge\n",
    "    insecureEdgeTerminationPolicy: Redirect\n",
    "  wildcardPolicy: None\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1158d5",
   "metadata": {},
   "source": [
    "Because there Data Science project already has some `NetworkPolicy` defined, we must also allow all `ingress` traffic to our workbench pod on the port we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbf092a-3ca5-4d02-8649-5e41f6a95f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "networkpolicy.networking.k8s.io/gpuaas-comfy unchanged\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "oc apply -f- << EOF\n",
    "kind: NetworkPolicy\n",
    "apiVersion: networking.k8s.io/v1\n",
    "metadata:\n",
    "  name: gpuaas-comfy\n",
    "  namespace: ai-roadshow\n",
    "spec:\n",
    "  podSelector:\n",
    "    matchLabels:\n",
    "      notebook-name: gpuaas\n",
    "  ingress:\n",
    "    - ports:\n",
    "        - protocol: TCP\n",
    "          port: 8188\n",
    "      from:\n",
    "        - namespaceSelector:\n",
    "            matchLabels:\n",
    "              network.openshift.io/policy-group: ingress\n",
    "  policyTypes:\n",
    "    - Ingress\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a223c3b6",
   "metadata": {},
   "source": [
    "We can check the Route URL in the OpenShift console, or directly from the cli.\n",
    "\n",
    "This is where we will connect using our web browser after we start ComfyUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2756e410-f5a8-4e16-8734-bc3326cc7fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME    HOST/PORT                                           PATH   SERVICES   PORT   TERMINATION     WILDCARD\n",
      "comfy   comfy-ai-roadshow.apps.sno.sandbox2964.opentlc.com          comfy      8188   edge/Redirect   None\n"
     ]
    }
   ],
   "source": [
    "!oc -n ai-roadshow get route comfy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0917fb",
   "metadata": {},
   "source": [
    "OK, using a bit of python - we can start up ComfyUI.\n",
    "\n",
    "This cell will continue running until you stop or restart it using the notebook controls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126a4d20-3f12-4cbf-9e2d-7f3b627708dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint files will always be loaded safely.\n",
      "Total VRAM 22599 MB, total RAM 15803 MB\n",
      "pytorch version: 2.7.1+cu126\n",
      "xformers version: 0.0.31\n",
      "Set vram state to: NORMAL_VRAM\n",
      "Device: cuda:0 NVIDIA A10G : cudaMallocAsync\n",
      "Using xformers attention\n",
      "Python version: 3.11.11 (main, Feb 10 2025, 00:00:00) [GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]\n",
      "ComfyUI version: 0.3.43\n",
      "****** User settings have been changed to be stored on the server instead of browser storage. ******\n",
      "****** For multi-user setups add the --multi-user CLI argument to enable multiple user profiles. ******\n",
      "ComfyUI frontend version: 1.23.4\n",
      "[Prompt Server] web root: /opt/app-root/lib64/python3.11/site-packages/comfyui_frontend_package/static\n",
      "\n",
      "Import times for custom nodes:\n",
      "   0.0 seconds: /opt/app-root/src/rhoai-roadshow/site/docs/6-gpuaas/notebooks/ComfyUI/custom_nodes/websocket_image_save.py\n",
      "\n",
      "Context impl SQLiteImpl.\n",
      "Will assume non-transactional DDL.\n",
      "No target revision found.\n",
      "Starting server\n",
      "\n",
      "To see the GUI go to: http://0.0.0.0:8188\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "\n",
    "def iframe_thread(port):\n",
    "  while True:\n",
    "      time.sleep(0.5)\n",
    "      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "      result = sock.connect_ex(('127.0.0.1', port))\n",
    "      if result == 0:\n",
    "        break\n",
    "      sock.close()\n",
    "  print(\"\\nComfyUI finished loading, trying to launch localtunnel (if it gets stuck here localtunnel is having issues)\\n\")\n",
    "\n",
    "  print(\"The password/enpoint ip for localtunnel is:\", urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n",
    "  p = subprocess.Popen([\"lt\", \"--port\", \"{}\".format(port)], stdout=subprocess.PIPE)\n",
    "  for line in p.stdout:\n",
    "    print(line.decode(), end='')\n",
    "\n",
    "\n",
    "threading.Thread(target=iframe_thread, daemon=True, args=(8188,)).start()\n",
    "\n",
    "!python main.py --listen 0.0.0.0 --port 8188 # --dont-print-server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de00981-d78d-4c9e-9ba6-c75e8726c9c2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Open up the ComfyUI (in this example https://comfy-ai-roadshow.apps.sno.sandbox2964.opentlc.com) using the Route URL from above. It should look something like this.\n",
    "\n",
    "![images/comfyui.png](images/comfyui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f140b6",
   "metadata": {},
   "source": [
    "If you do not see a workflow .. you can easily create one by **drag-n-droping** this image - `images/ComfyUI_00005_.png` into the workflow webpage. \n",
    "\n",
    "Set the `Load Checkpoint` to be the safetensor model `sd_xl_base_1.0.safetensors` we downloaded earlier:\n",
    "\n",
    "![images/comfyui-checkpoint.png](images/comfyui-checkpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdd03a2",
   "metadata": {},
   "source": [
    "You can also change the `prompt` used to generate the image.\n",
    "\n",
    "![images/comfyui-prompt.png](images/comfyui-prompt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a1ab03",
   "metadata": {},
   "source": [
    "If you hit the `Run` button, Comfyui will load the prompt, the model and generate an image. This may a minute or two for the first run (subsequent runs should be quicker).\n",
    "\n",
    "You can check the GPU is in use by running `nvtop` from the `tools` pod Terminal.\n",
    "\n",
    "![images/nvtop-comfyui.png](images/nvtop-comfyui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5086fc6e",
   "metadata": {},
   "source": [
    "The CompfyUI workflow should complete and output an image to the `ComfyUI/output` folder.\n",
    "\n",
    "![images/comfyui-success.png](images/comfyui-success.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0a389e",
   "metadata": {},
   "source": [
    "If the python kernel dies, it may be your pod needs more RAM - check the metrics in OpenShift to find out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271de4ec",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Success:</b> Our marketing department can generate images for their new marketing campaign !!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd93528",
   "metadata": {},
   "source": [
    "Continue to the [next notebook](./Level4_advanced_gpuaas.ipynb) to learn about distributing workloads (this may change in the future).\n",
    "\n",
    "Else try the [Level5 notebook](./Level5_multi-gpu-node.ipynb) to configure a larger LLM across multi-node, multi-gpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9296dab1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
