{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e687dc7-84a4-4761-ac03-921be2dc5b98",
   "metadata": {},
   "source": [
    "## Running a larger LLM on multiple GPU and multiple Nodes\n",
    "\n",
    "If you jumped to here from Level4 notebook then carry on ! ü™è\n",
    "\n",
    "The notebook is based partly on the product documentation with some enhancements. Some useful Links.\n",
    "\n",
    "- https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.22/html/serving_models/serving-large-models_serving-large-models#deploying-models-using-multiple-gpu-nodes_serving-large-models\n",
    "- https://access.redhat.com/articles/6966373\n",
    "- https://github.com/rh-aiservices-bu/multi-node-multi-gpu-poc\n",
    "\n",
    "### GPU Aggregation Overview\n",
    "\n",
    "Compute workloads can benefit from using separate GPU partitions. The flexibility of GPU partitioning allows a single GPU to be shared and used by small, medium, and large-sized workloads. GPU partitions can be a valid option for executing Deep Learning workloads. An example is Deep Learning training and inferencing workflows, which utilize smaller datasets but are highly dependent on the size of the data/model, and users may need to decrease batch sizes.\n",
    "\n",
    "#### Why GPU Aggregation?\n",
    "\n",
    "Some Large Language Models (LLMs), such as Llama-3-70B and Falcon 180B, can be too large to fit into the memory of a single GPU (vRAM). Or in some cases, GPUs that would be large-enough might be difficult to obtain. If you find yourself in such a situation, it is natural to wonder whether an aggregation of multiple, smaller GPUs can be used instead of one single large GPU.\n",
    "\n",
    "Thankfully, the answer is essentially Yes. To address these challenges, we can use more advanced configurations to distribute the LLM workload across several GPUs. One option is leveraging tensor parallelism, where the LLM is split across several GPUs, with each GPU processing a portion of the model's tensors. This approach ensures efficient utilization of available resources (GPUs) across one or several workers.\n",
    "\n",
    "Some Serving Runtimes, such as vLLM, support tensor parallelism, allowing for both single-worker and multi-worker configurations (the difference whether your GPUs are all in the same machine, or are spread across machines).\n",
    "\n",
    "#### Components of GPU Aggregation\n",
    "\n",
    "GPU Aggregation is a complex topic and there are many components to consider. Fundamentally there are four core concepts to consider: \n",
    "\n",
    "* Tensor Parallelism\n",
    "* Pipeline Parallelism\n",
    "* Data Parallelism\n",
    "* Expert Parallelism\n",
    "\n",
    "In tensor parallelism, each GPU processes a slice of a tensor and only aggregates the full tensor when necessary for specific operations. This approach allows larger models to run efficiently across multiple devices while maintaining performance.\n",
    "\n",
    "Pipeline parallelism differs from tensor parallelism in that it splits the model vertically (across layers) rather than horizontally (across tensor dimensions). Each GPU or node in the pipeline processes a complete subset of the model's layers, making it particularly effective for extremely large models like DeepSeek R1 or Llama 3.1 405B that cannot fit on a single node.\n",
    "\n",
    "Data Parallelism (DP) replicates the model across multiple GPUs. Data batches are evenly distributed between GPUs and the data-parallel GPUs process them independently. While the computation workload is efficiently distributed across GPUs, inter-GPU communication is required in order to keep the model replicas consistent between training steps.\n",
    "\n",
    "Expert parallelism is a specialized distributed computing technique designed specifically for Mixture of Experts (MoE) models. Unlike traditional parallelism strategies that distribute computation across all model parameters, expert parallelism leverages the sparse activation pattern of MoE architectures where only a subset of experts are activated for each input token.\n",
    "\n",
    "<img src=\"images/gpu-aggregation.png\"\n",
    "     alt=\"GPU Aggregation\"\n",
    "     style=\"width:75%;\">\n",
    "\n",
    "### In this lab\n",
    "\n",
    "We are going to deploy a larger LLM across both our GPU enabled nodes. This needs both GPUs in full to run.\n",
    "\n",
    "#### üí° Free up GPU memory to run this exercise\n",
    "\n",
    "We can stop the vLLM inference model servers that are running in the namespace `llama-serving`.\n",
    "\n",
    "Browse to the Models > Model deployments page in Red Hat OpenShift AI Web Console. **Stop** both the model deployments.\n",
    "\n",
    "![images/model-serving-stop-start.png](images/model-serving-stop-start.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e21e6d6",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Note:** If you need to redeploy these models then simply start them up again. The **order** you start them up **matters** for correct startup. \n",
    "\n",
    "Deploy Llama3 first, then DeepSeek second.\n",
    "\n",
    "We have limited GPU NVRAM so we use vLLMs `gpu_memory_utilization` parameter when loading the models. This works on available GPU memory so we need to load Llama3 (gpu_memory_utilization=0.5) first then DeepSeek (gpu_memory_utilization=0.8) second.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e783d636",
   "metadata": {},
   "source": [
    "### Configure RWX Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c38052",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Using insecure TLS client config. Setting this option is not supported!\n",
      "\n",
      "Login successful.\n",
      "\n",
      "You have access to 115 projects, the list has been suppressed. You can list all projects with 'oc projects'\n",
      "\n",
      "Using project \"ai-roadshow\".\n"
     ]
    }
   ],
   "source": [
    "!oc login -u admin -p ${ADMIN_PASSWORD} --server=https://api.${BASE_DOMAIN}:6443 --insecure-skip-tls-verify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2f4524",
   "metadata": {},
   "source": [
    "Check we have the **efs-sc** storage class configured. If not - check with your cluster admin !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18d0d6ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME     PROVISIONER       RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE\n",
      "efs-sc   efs.csi.aws.com   Delete          Immediate           false                  19h\n"
     ]
    }
   ],
   "source": [
    "!oc get sc efs-sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8083a173",
   "metadata": {},
   "source": [
    "### Download Larger Model for Inference to Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2e0091-745d-4c8a-a6eb-0328e921bca1",
   "metadata": {},
   "source": [
    "For demonstration purpoeses - let's select a model that we know will not fit on our single 24Gi GPU. Lets try RedHatAI/Mistral-Small-24B-Instruct-2501-FP8-dynamic which is a good quality quantized model that has ~30Gi of safetensor weights and will also need KV cache - so will definitely not fit on our single GPU.\n",
    "\n",
    "https://huggingface.co/RedHatAI/Mistral-Small-24B-Instruct-2501-FP8-dynamic/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19156858-628f-4940-a31e-56232fc1ed67",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!oc new-project kserve-demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a454be4f",
   "metadata": {},
   "source": [
    "Download the model into our PVC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6b03590",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MODEL_PATH=mistral-small\n"
     ]
    }
   ],
   "source": [
    "%env MODEL_PATH=mistral-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "981d5405-b15b-4485-a9b6-66afc1134fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persistentvolumeclaim/mistral-small-pvc created\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "oc apply -f- << EOF\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: ${MODEL_PATH}-pvc\n",
    "spec:\n",
    "  accessModes:\n",
    "    - ReadWriteMany\n",
    "  volumeMode: Filesystem\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 30Gi\n",
    "  storageClassName: efs-sc\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111eb33f-0a74-4e18-8542-8a4dfee3bf43",
   "metadata": {},
   "source": [
    "Lets grab a YAML file that will help us download the Hugging Face model to a PVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9495d158-b223-4a0e-b3fb-ffcf07b5cf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1552  100  1552    0     0   5602      0 --:--:-- --:--:-- --:--:--  5582\n"
     ]
    }
   ],
   "source": [
    "!curl -o download-model-to-pvc.yaml https://raw.githubusercontent.com/eformat/rhoai-policy-collection/refs/heads/main/gitops/applications/model-download/download-model-to-pvc.yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc4b9e3-ed05-49a8-8b6d-c97751c72879",
   "metadata": {},
   "source": [
    "Make sure to set your Hugging Face token **HF_TOKEN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c9b209-609d-4465-ba75-518f3ae5326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PVC_CLAIM_NAME=mistral-small-pvc\n",
    "%env HF_TOKEN=hf_\n",
    "%env MODEL=RedHatAI/Mistral-Small-24B-Instruct-2501-FP8-dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a86572-17e0-4cde-9892-177bad3b0be1",
   "metadata": {},
   "source": [
    "Now create the downloader pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b60e4ff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/download-model created\n"
     ]
    }
   ],
   "source": [
    "!cat download-model-to-pvc.yaml | envsubst | oc apply -f-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dde7fc3-44e2-48c3-8c18-60f6883c2c05",
   "metadata": {},
   "source": [
    "Wait until pod completes successfully ~apprx 6-8min\n",
    "\n",
    "Follow the logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32d6821c-bb36-493e-a5d7-618f913f634e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/app-root/lib64/python3.11/site-packages (24.2)\n",
      "Collecting pip\n",
      "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1.8/1.8 MB 122.4 MB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.2\n",
      "    Uninstalling pip-24.2:\n",
      "      Successfully uninstalled pip-24.2\n",
      "Successfully installed pip-25.2\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting filelock (from huggingface_hub)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface_hub)\n",
      "  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting packaging>=20.9 (from huggingface_hub)\n",
      "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pyyaml>=5.1 (from huggingface_hub)\n",
      "  Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting requests (from huggingface_hub)\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tqdm>=4.42.1 (from huggingface_hub)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface_hub)\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub)\n",
      "  Downloading hf_xet-1.1.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (703 bytes)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->huggingface_hub)\n",
      "  Downloading charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->huggingface_hub)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->huggingface_hub)\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->huggingface_hub)\n",
      "  Downloading certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 561.5/561.5 kB 205.4 MB/s  0:00:00\n",
      "Downloading hf_xet-1.1.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3.2/3.2 MB 374.4 MB/s  0:00:00\n",
      "Downloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 763.0/763.0 kB 468.2 MB/s  0:00:00\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (150 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, tqdm, pyyaml, packaging, idna, hf-xet, fsspec, filelock, charset_normalizer, certifi, requests, huggingface_hub\n",
      "\n",
      "Successfully installed certifi-2025.8.3 charset_normalizer-3.4.3 filelock-3.18.0 fsspec-2025.7.0 hf-xet-1.1.7 huggingface_hub-0.34.4 idna-3.10 packaging-25.0 pyyaml-6.0.2 requests-2.32.4 tqdm-4.67.1 typing-extensions-4.14.1 urllib3-2.5.0\n",
      "/opt/app-root/lib64/python3.11/site-packages/huggingface_hub/file_download.py:982: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n",
      "Fetching 15 files:   7%|‚ñã         | 1/15 [00:01<00:14,  1.04s/it]"
     ]
    }
   ],
   "source": [
    "!oc -n kserve-demo logs -c download-model download-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918ce73b-6d80-4e2f-82ff-29a768bab747",
   "metadata": {},
   "source": [
    "Wait till download completes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c569fd",
   "metadata": {},
   "source": [
    "### Create Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bd3f07-edb0-4d9c-b95e-08649caf57ed",
   "metadata": {},
   "source": [
    "RHOAI comes with the templates needed to run multinode multigpu, lets use them to create the ServingRuntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a486c199",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "servingruntime.serving.kserve.io/vllm-multinode-runtime created\n"
     ]
    }
   ],
   "source": [
    "!oc process vllm-multinode-runtime-template -n redhat-ods-applications | oc apply -n kserve-demo -f-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5691cb72",
   "metadata": {},
   "source": [
    "The important part of the template is the GPU Aggregation and Sharing settings.\n",
    "\n",
    "```yaml\n",
    "      pipelineParallelSize: 2  # the number of nodes we have\n",
    "      tensorParallelSize: 1    # the number of GPUs that are available for vLLM on a node\n",
    "```\n",
    "\n",
    "Let's create the inference service now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eaed35-a722-430d-ab99-bf154aef9dc6",
   "metadata": {},
   "source": [
    "The templates have hard limits which are pretty excessive for our resources, trim them so we only set QoS to burstable i.e. set requests only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2686716f-a7bc-40ba-9443-ac039c437e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "servingruntime.serving.kserve.io/vllm-multinode-runtime patched\n"
     ]
    }
   ],
   "source": [
    "!oc patch servingruntime vllm-multinode-runtime -n kserve-demo --type='json' -p='[{\"op\": \"remove\", \"path\": \"/spec/containers/0/resources\"}]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e519ad91-1afc-4420-a40a-d13aa175e08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "servingruntime.serving.kserve.io/vllm-multinode-runtime patched\n"
     ]
    }
   ],
   "source": [
    "!oc patch servingruntime vllm-multinode-runtime -n kserve-demo --type='json' -p='[{\"op\": \"add\", \"path\": \"/spec/containers/0/resources\", \"value\": {\"requests\":{\"cpu\":\"1\",\"memory\":\"2Gi\"}}}]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec4121cc-9ce7-485d-85fa-98945a591686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "servingruntime.serving.kserve.io/vllm-multinode-runtime patched\n"
     ]
    }
   ],
   "source": [
    "!oc patch servingruntime vllm-multinode-runtime -n kserve-demo --type='json' -p='[{\"op\": \"remove\", \"path\": \"/spec/workerSpec/containers/0/resources\"}]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dcdfb9e0-0216-40b4-ae5f-63eb421a835e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "servingruntime.serving.kserve.io/vllm-multinode-runtime patched\n"
     ]
    }
   ],
   "source": [
    "!oc patch servingruntime vllm-multinode-runtime -n kserve-demo --type='json' -p='[{\"op\": \"add\", \"path\": \"/spec/workerSpec/containers/0/resources\", \"value\": {\"requests\":{\"cpu\":\"1\",\"memory\":\"2Gi\"}}}]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7cd5ac2c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: INFERENCE_NAME=mistral-small\n",
      "env: MODEL_PATH=mistral-small\n"
     ]
    }
   ],
   "source": [
    "%env INFERENCE_NAME=mistral-small\n",
    "%env MODEL_PATH=mistral-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f36842f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io/mistral-small created\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "oc apply -f- << EOF\n",
    "apiVersion: serving.kserve.io/v1beta1\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  annotations:\n",
    "    serving.kserve.io/deploymentMode: RawDeployment\n",
    "    serving.kserve.io/autoscalerClass: none\n",
    "  name: ${INFERENCE_NAME}\n",
    "spec:\n",
    "  predictor:\n",
    "    model:\n",
    "      modelFormat:\n",
    "        name: vLLM\n",
    "      runtime: vllm-multinode-runtime\n",
    "      storageUri: pvc://${PVC_CLAIM_NAME}/${MODEL_PATH}\n",
    "    workerSpec: {}\n",
    "    tolerations:\n",
    "      - effect: NoSchedule\n",
    "        key: nvidia.com/gpu\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55347e3-74ef-48aa-a68f-962e8c3605fc",
   "metadata": {},
   "source": [
    "Tail the logs on the inference pod, we should see the safetensor shards loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5c2ba0-15cc-4e35-a7e8-9b2bed0bf5ae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```bash\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:25:53 [ray_distributed_executor.py:357] If certain env vars should NOT be copied to workers, add them to /tmp/.config/vllm/ray_non_carry_over_env_vars.json file\n",
    "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]pid=912) \n",
    "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:10<00:50, 10.05s/it] \n",
    "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:10<00:16,  4.24s/it] \n",
    "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:13<00:10,  3.61s/it] \n",
    "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:20<00:10,  5.01s/it] \n",
    "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:22<00:04,  4.04s/it] \n",
    "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:32<00:00,  6.14s/it] \n",
    "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:32<00:00,  5.46s/it]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a380609-dc0e-46cd-b493-668c3e9dd025",
   "metadata": {},
   "source": [
    "After some time the OpenAI API becomes ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998911c8-4d4a-4a7d-8f50-3cd62a75fe2c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```bash\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:28] Available routes are:\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /openapi.json, Methods: HEAD, GET\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /docs, Methods: HEAD, GET\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /redoc, Methods: HEAD, GET\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /health, Methods: GET\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /load, Methods: GET\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /ping, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /ping, Methods: GET\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /tokenize, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /detokenize, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /v1/models, Methods: GET\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /version, Methods: GET\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /v1/chat/completions, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /v1/completions, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /v1/embeddings, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /pooling, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /classify, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /score, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /v1/score, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /rerank, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /v1/rerank, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /v2/rerank, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /invocations, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /metrics, Methods: GET\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO:     Started server process [1]\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO:     Waiting for application startup.\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO:     Application startup complete.\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO:     10.128.0.218:55526 - \"GET /metrics HTTP/1.1\" 200 OK\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbca3ec-55b9-4481-9848-3e0a6758d97f",
   "metadata": {},
   "source": [
    "Check Pod Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "753caa9e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                           READY   STATUS    RESTARTS   AGE    IP             NODE                                             NOMINATED NODE   READINESS GATES\n",
      "mistral-small-predictor-587648f5c4-97vf5       0/1     Running   0          2m2s   10.129.0.54    ip-10-0-94-103.ap-southeast-2.compute.internal   <none>           <none>\n",
      "mistral-small-predictor-worker-dcc588b-95xdc   0/1     Running   0          2m2s   10.128.1.166   ip-10-0-84-186.ap-southeast-2.compute.internal   <none>           <none>\n"
     ]
    }
   ],
   "source": [
    "!oc get pods -o wide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b74949a-53e6-4b12-9693-ae6d1bd72901",
   "metadata": {},
   "source": [
    "We can also check nvidia-smi for GPU NVRAM usage stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "353d7834",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DEMO_NAMESPACE=kserve-demo\n",
      "env: MODEL_NAME=mistral-small\n"
     ]
    }
   ],
   "source": [
    "%env DEMO_NAMESPACE=kserve-demo\n",
    "%env MODEL_NAME=mistral-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5dbcc612",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/mistral-small-predictor-587648f5c4-97vf5 condition met\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "podName=$(oc get pod -n $DEMO_NAMESPACE -l app=isvc.$MODEL_NAME-predictor --no-headers|cut -d' ' -f1)\n",
    "workerPodName=$(oc get pod -n $DEMO_NAMESPACE -l app=isvc.$MODEL_NAME-predictor-worker --no-headers|cut -d' ' -f1)\n",
    "oc -n $DEMO_NAMESPACE wait --for=condition=ready pod/${podName} --timeout=300s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd8e5a8-5c2a-442d-846e-dfc4194d88df",
   "metadata": {},
   "source": [
    "We can see model loaded across both of out GPU nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b8a10786",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### HEAD NODE GPU Memory Size\n",
      "Wed Aug 13 01:55:38 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A10G                    On  |   00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   34C    P0             92W /  300W |   20751MiB /  23028MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A             402      C   ray::RayWorkerWrapper                 20742MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "### Worker NODE GPU Memory Size\n",
      "Wed Aug 13 01:55:39 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA L4                      On  |   00000000:36:00.0 Off |                    0 |\n",
      "| N/A   46C    P0             33W /   72W |   20936MiB /  23034MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A             488      C   ray::RayWorkerWrapper                 20928MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "podName=$(oc get pod -n $DEMO_NAMESPACE -l app=isvc.$MODEL_NAME-predictor --no-headers|cut -d' ' -f1)\n",
    "workerPodName=$(oc get pod -n $DEMO_NAMESPACE -l app=isvc.$MODEL_NAME-predictor-worker --no-headers|cut -d' ' -f1)\n",
    "echo \"### HEAD NODE GPU Memory Size\"\n",
    "oc -n $DEMO_NAMESPACE exec $podName -c kserve-container -- nvidia-smi\n",
    "echo \"### Worker NODE GPU Memory Size\"\n",
    "oc -n $DEMO_NAMESPACE exec $workerPodName -c worker-container -- nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca8893e-03dd-417b-b0ca-ef0da1ddb8e3",
   "metadata": {},
   "source": [
    "Lets create a route so we can test the inference endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6ea8a3c3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "route.route.openshift.io/mistral-small created\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "oc apply -f- << EOF\n",
    "kind: Route\n",
    "apiVersion: route.openshift.io/v1\n",
    "metadata:\n",
    "  name: ${INFERENCE_NAME}\n",
    "  labels:\n",
    "    app: isvc.${INFERENCE_NAME}-predictor\n",
    "    component: predictor\n",
    "    isvc.generation: \"1\"\n",
    "    serving.kserve.io/inferenceservice: ${INFERENCE_NAME}\n",
    "  annotations:\n",
    "    openshift.io/host.generated: \"true\"\n",
    "spec:\n",
    "  to:\n",
    "    kind: Service\n",
    "    name: ${INFERENCE_NAME}-predictor\n",
    "    weight: 100\n",
    "  port:\n",
    "    targetPort: http\n",
    "  tls:\n",
    "    termination: edge\n",
    "    insecureEdgeTerminationPolicy: Redirect\n",
    "  wildcardPolicy: None\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7fe109-c266-476f-9c8c-a1c1d1158488",
   "metadata": {},
   "source": [
    "Check endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6c523800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"cmpl-b91ccade2c334f46b8d1e28d3079fad1\",\n",
      "    \"object\": \"text_completion\",\n",
      "    \"created\": 1755050407,\n",
      "    \"model\": \"mistral-small\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"index\": 0,\n",
      "            \"text\": \" The answer is not Mount Everest. The biggest mountain in the world is actually Mauna Kea in Hawaii. While Mount Everest is the highest peak above sea level, Mauna Kea is the tallest when measured from base to peak. Mauna Kea is a dormant volcano that rises about 33,500 feet (10,210 meters) from its base on the ocean floor to its peak, which is 13,796 feet (\",\n",
      "            \"logprobs\": null,\n",
      "            \"finish_reason\": \"length\",\n",
      "            \"stop_reason\": null,\n",
      "            \"prompt_logprobs\": null\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 10,\n",
      "        \"total_tokens\": 110,\n",
      "        \"completion_tokens\": 100,\n",
      "        \"prompt_tokens_details\": null\n",
      "    },\n",
      "    \"kv_transfer_params\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "isvc_url=$(oc get route -n $DEMO_NAMESPACE |grep $MODEL_NAME| awk '{print $2}')\n",
    "\n",
    "curl -s https://$isvc_url/v1/completions \\\n",
    "   -H \"Content-Type: application/json\" \\\n",
    "   -d \"{\n",
    "        \\\"model\\\": \\\"$MODEL_NAME\\\",\n",
    "        \\\"prompt\\\": \\\"What is the biggest mountain in the world?\\\",\n",
    "        \\\"max_tokens\\\": 100,\n",
    "        \\\"temperature\\\": 0\n",
    "    }\" | python -m json.tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf66369-5f67-4022-827e-7ed0ef33d6b2",
   "metadata": {},
   "source": [
    "Mauna Kea indeed !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d3d587-1877-4ae6-a277-5193d91abe9f",
   "metadata": {},
   "source": [
    "Multi-node vLLM uses [Ray](https://www.ray.io) to distribute the model across multiple nodes and vLLM manages the Ray instance for you.\n",
    "\n",
    "The multi-node vLLM distribution does not depend on any external Ray instances or RHOAI's distributed training tooling such as KubeRay or CodeFlare.\n",
    "\n",
    "We can check the ray status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "abb58552",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Defaulted container \"kserve-container\" out of: kserve-container, ray-tls-generator (init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Autoscaler status: 2025-08-13 02:00:58.234015 ========\n",
      "Node status\n",
      "---------------------------------------------------------------\n",
      "Active:\n",
      " 1 node_e05ad693f65a468ac50a6829ae2a508bdbb1f4e5196c3a4c830f6a14\n",
      " 1 node_622ab5f41cc58269dba5009434f8f1a6989bb824b43ed297054d1338\n",
      "Pending:\n",
      " (no pending nodes)\n",
      "Recent failures:\n",
      " (no failures)\n",
      "\n",
      "Resources\n",
      "---------------------------------------------------------------\n",
      "Total Usage:\n",
      " 0.0/36.0 CPU\n",
      " 2.0/2.0 GPU (2.0 used of 2.0 reserved in placement groups)\n",
      " 0B/120.06GiB memory\n",
      " 0B/15.86GiB object_store_memory\n",
      "\n",
      "Total Constraints:\n",
      " (no request_resources() constraints)\n",
      "Total Demands:\n",
      " (no resource demands)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "podName=$(oc get pod -n $DEMO_NAMESPACE -l app=isvc.$MODEL_NAME-predictor --no-headers|cut -d' ' -f1)\n",
    "oc exec -i pod/${podName} -- /bin/sh -s << EOF\n",
    "ray status\n",
    "EOF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
