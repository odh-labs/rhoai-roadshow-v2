{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6126afe6-2eaf-4830-8946-db33701f0021",
   "metadata": {},
   "source": [
    "# Install requried Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c05621ab-73fd-4249-9c1d-41bdabe98f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: uv in /opt/app-root/lib64/python3.11/site-packages (0.8.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install necessary Python libraries\n",
    "\n",
    "#!pip install -q -r requirements.txt\n",
    "!pip install uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ae9b1db-5d43-4d2f-a4c3-cb11879d2d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.11 environment at: /opt/app-root\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m5 packages\u001b[0m \u001b[2min 31ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Use uv to install the dependencies\n",
    "!uv pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816002df-d936-4529-84da-58e59bdf1a12",
   "metadata": {},
   "source": [
    "# Log in to the OpenShift cluster\n",
    "\n",
    "In this lab you will interact directly with the OpenShift cluster. To do that you must first log in to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba04707-049d-4732-96eb-a559a022f5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful.\n",
      "\n",
      "You have access to 109 projects, the list has been suppressed. You can list all projects with 'oc projects'\n",
      "\n",
      "Using project \"vllm-demo\".\n"
     ]
    }
   ],
   "source": [
    "!oc login -u admin -p ${ADMIN_PASSWORD} --server=https://api.${BASE_DOMAIN}:6443"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9c9fc6-cc09-44a4-b7b4-2086952f04c6",
   "metadata": {},
   "source": [
    "# 💡 Free up GPU memory to run this exercise\n",
    "\n",
    "Our `Lab 1 - inference with vllm` is quite computationally intensive. This workshop is only using an L4 GPU so in order ensure the success of jupyterlab notebooks.\n",
    "\n",
    "We can stop the vLLM inference model servers that are running in the namespace `llama-serving`.\n",
    "\n",
    "Browse to the Models > Model deployments page in Red Hat OpenShift AI Web Console. **Stop** both the model deployments.\n",
    "\n",
    "![images/model-serving-stop-start.png](images/model-serving-stop-start.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711d8217",
   "metadata": {},
   "source": [
    "⚠️ **Note:** If you need to redeploy these models then simply start them up again. The **order** you start them up **matters** for correct startup. \n",
    "\n",
    "Deploy Llama3 first, then DeepSeek second.\n",
    "\n",
    "We have limited GPU NVRAM so we use vLLMs `gpu_memory_utilization` parameter when loading the models. This works on available GPU memory so we need to load Llama3 (gpu_memory_utilization=0.5) first then DeepSeek (gpu_memory_utilization=0.8) second.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64c70b1-9238-4bf2-9837-5e0601a3ca77",
   "metadata": {},
   "source": [
    "# 🔍 Environment Validation Script\n",
    "\n",
    "This script validates the environment by checking:\n",
    "- vllm version (expected: 0.9.1)\n",
    "- llmcompressor version (expected: 0.5.2)\n",
    "- llama-serving namespace status\n",
    "\n",
    "⚠️ **Note**: You can safely ignore the following \"Deprecation Warning\" when you run this step.  \n",
    "*/tmp/ipykernel_1203/152499475.py:3: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b867e95a-a7cc-4a52-9090-a540c9d2d7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_961/152499475.py:3: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Environment Validation Report\n",
      "==================================================\n",
      "✅ vllm: 0.9.1 (>= 0.9.1)\n",
      "✅ llmcompressor: 0.5.2 (>= 0.5.2)\n",
      "\n",
      "🔍 llama-serving Status:\n",
      "------------------------------\n",
      "✅ llama-serving namespace is accessible\n",
      "  📦 llama3-2-3b-predictor-b49576976-6krvj: 0/2 PodInitializing\n",
      "  📦 sno-deepseek-qwen3-vllm-predictor-694dd7ff5-8nvlm: 1/2 Running\n",
      "\n",
      "==================================================\n",
      "🎉 All validation checks passed! Environment is ready.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import pkg_resources\n",
    "from packaging import version\n",
    "import os\n",
    "\n",
    "def check_package_version(package_name, expected_version):\n",
    "    \"\"\"Check if a package is installed and has the expected version\"\"\"\n",
    "    try:\n",
    "        installed_version = pkg_resources.get_distribution(package_name).version\n",
    "        if version.parse(installed_version) >= version.parse(expected_version):\n",
    "            return True, installed_version\n",
    "        else:\n",
    "            return False, installed_version\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        return False, \"Not installed\"\n",
    "\n",
    "def check_llama_serving_status():\n",
    "    \"\"\"Check the status of llama-serving namespace\"\"\"\n",
    "    try:\n",
    "        # Check if we can access the namespace\n",
    "        result = subprocess.run( \n",
    "            [\"oc\", \"get\", \"pods\", \"-n\", \"llama-serving\", \"--no-headers\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            lines = result.stdout.strip().split('\\n')\n",
    "            if not lines or lines == ['']:\n",
    "                return True, \"No pods found - default vLLM inference serving is disabled (optimal for labs)\"\n",
    "            \n",
    "            # Check pod status\n",
    "            pod_status = []\n",
    "            for line in lines:\n",
    "                if line.strip():\n",
    "                    parts = line.split()\n",
    "                    if len(parts) >= 3:\n",
    "                        pod_name = parts[0]\n",
    "                        ready = parts[1]\n",
    "                        status = parts[2]\n",
    "                        pod_status.append(f\"{pod_name}: {ready} {status}\")\n",
    "            \n",
    "            return True, pod_status\n",
    "        else:\n",
    "            return False, f\"Error accessing namespace: {result.stderr}\"\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return False, \"Timeout accessing llama-serving namespace\"\n",
    "    except Exception as e:\n",
    "        return False, f\"Error: {str(e)}\"\n",
    "\n",
    "def validate_environment():\n",
    "    \"\"\"Main validation function\"\"\"\n",
    "    print(\"🔍 Environment Validation Report\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check required packages\n",
    "    required_packages = {\n",
    "        \"vllm\": \"0.9.1\",\n",
    "        \"llmcompressor\": \"0.5.2\"\n",
    "    }\n",
    "    \n",
    "    all_checks_passed = True\n",
    "    \n",
    "    for package, expected_ver in required_packages.items():\n",
    "        is_valid, installed_ver = check_package_version(package, expected_ver)\n",
    "        \n",
    "        if is_valid:\n",
    "            print(f\"✅ {package}: {installed_ver} (>= {expected_ver})\")\n",
    "        else:\n",
    "            print(f\"❌ {package}: {installed_ver} (expected >= {expected_ver})\")\n",
    "            all_checks_passed = False\n",
    "    \n",
    "    # Check llama-serving status\n",
    "    print(\"\\n🔍 llama-serving Status:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    is_accessible, status_info = check_llama_serving_status()\n",
    "    \n",
    "    if is_accessible:\n",
    "        print(\"✅ llama-serving namespace is accessible\")\n",
    "        if isinstance(status_info, list):\n",
    "            for pod_info in status_info:\n",
    "                print(f\"  📦 {pod_info}\")\n",
    "        else:\n",
    "            print(f\"  ℹ️  {status_info}\")\n",
    "    else:\n",
    "        print(f\"❌ llama-serving namespace: {status_info}\")\n",
    "        all_checks_passed = False\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    if all_checks_passed:\n",
    "        print(\"🎉 All validation checks passed! Environment is ready.\")\n",
    "    else:\n",
    "        print(\"⚠️  Some validation checks failed. Please review the issues above.\")\n",
    "    \n",
    "    return all_checks_passed\n",
    "\n",
    "# Run the validation\n",
    "validate_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846879af-205a-41d0-b3db-25285fa69e8a",
   "metadata": {},
   "source": [
    "## Quick Status Check\n",
    "\n",
    "Run the cell below for a quick status summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d98b3031-c505-4fa8-b010-7ab98d06d021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Quick Environment Status Check\n",
      "===================================\n",
      "✅ vllm: 0.9.1\n",
      "✅ llmcompressor: 0.5.2\n",
      "✅ llama-serving: accessible, default vLLM inference serving is disabled (optimal for labs)\n",
      "\n",
      "🎉 Overall: All systems ready!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def quick_status_check():\n",
    "    \"\"\"Quick validation check with minimal output\"\"\"\n",
    "    print(\"🚀 Quick Environment Status Check\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Check vllm\n",
    "    vllm_ok, vllm_ver = check_package_version(\"vllm\", \"0.9.1\")\n",
    "    print(f\"{'✅' if vllm_ok else '❌'} vllm: {vllm_ver}\")\n",
    "    \n",
    "    # Check llmcompressor\n",
    "    llmc_ok, llmc_ver = check_package_version(\"llmcompressor\", \"0.5.2\")\n",
    "    print(f\"{'✅' if llmc_ok else '❌'} llmcompressor: {llmc_ver}\")\n",
    "    \n",
    "    # Check llama-serving\n",
    "    llama_ok, _ = check_llama_serving_status()\n",
    "    print(f\"{'✅' if llama_ok else '❌'} llama-serving: {'accessible, default vLLM inference serving is disabled (optimal for labs)' if llama_ok else 'issue detected'}\")\n",
    "    \n",
    "    # Overall status\n",
    "    all_good = vllm_ok and llmc_ok and llama_ok\n",
    "    print(f\"\\n{'🎉' if all_good else '⚠️'} Overall: {'All systems ready!' if all_good else 'Issues found - run full validation above'}\")\n",
    "    \n",
    "    return all_good\n",
    "\n",
    "# Run quick check\n",
    "quick_status_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298777a4-2555-4a1d-88dc-fb3d8cd86254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
