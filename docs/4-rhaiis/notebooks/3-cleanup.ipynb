{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf3242b8-f138-47ae-af52-3db86a927785",
   "metadata": {},
   "source": [
    "# üßπ Clean Up the enviornment for the rest labs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaf984b-baee-4cca-8b04-b58767184a53",
   "metadata": {},
   "source": [
    "After completing this module, we will need to bring back the default served model in the namespace `llama-serving` for the rest of the hands-on lab.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad02aaa6-91ca-4e79-a121-d4875973986e",
   "metadata": {},
   "source": [
    "### üí° Log in to the OpenShift cluster\n",
    "\n",
    "In this lab you will interact directly with the OpenShift cluster. To do that you must first log in to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b54d74-ecce-4b2e-b3f3-440688adc84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful.\n",
      "\n",
      "You have access to 116 projects, the list has been suppressed. You can list all projects with 'oc projects'\n",
      "\n",
      "Using project \"ai-roadshow\".\n",
      "Welcome! See 'oc help' to get started.\n"
     ]
    }
   ],
   "source": [
    "!oc login -u admin -p ${ADMIN_PASSWORD} --server=https://api.${BASE_DOMAIN}:6443"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d9343c-b392-45de-9410-9608f1c2c2c5",
   "metadata": {},
   "source": [
    "### üîç Redeploy the default models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ccb1da-1a98-461b-a33e-774192cb49ba",
   "metadata": {},
   "source": [
    "We can start the vLLM inference model servers that are running in the namespace `llama-serving`.\n",
    "\n",
    "Browse to the Models > Model deployments page in Red Hat OpenShift AI Web Console. **Start** both the model deployments.\n",
    "\n",
    "‚ö†Ô∏è **Note:** The **order** you start them up **matters** for correct startup. \n",
    "\n",
    "Deploy Llama3 first, then DeepSeek second.\n",
    "\n",
    "We have limited GPU NVRAM so we use vLLMs `gpu_memory_utilization` parameter when loading the models. This works on available GPU memory so we need to load Llama3 (gpu_memory_utilization=0.5) first then DeepSeek (gpu_memory_utilization=0.8) second.\n",
    "\n",
    "![images/model-serving-stop-start.png](images/model-serving-stop-start.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360cb2ac-5ec8-4722-9dd6-eb659fead5f4",
   "metadata": {},
   "source": [
    "### ‚úÖ Double check those default models are now back to be served"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc3557c6-f1ae-4407-9240-186e25d54433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                READY   STATUS    RESTARTS   AGE\n",
      "llama3-2-3b-predictor-b49576976-ql2zm               2/2     Running   0          2m45s\n",
      "sno-deepseek-qwen3-vllm-predictor-694dd7ff5-wq7gb   2/2     Running   0          2m26s\n"
     ]
    }
   ],
   "source": [
    "!oc get pods -n llama-serving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fb63e6-fc1d-4f5f-bde3-e16ecb3694de",
   "metadata": {},
   "source": [
    "---\n",
    "#### üéâ Congratulations! You have now completed all the modules in the Red Hat AI Inference Server! Please move on to other modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea32afee-ca8c-4159-a20b-5cfd0b8f9799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
