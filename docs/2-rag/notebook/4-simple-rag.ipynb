{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac9b9a5e-7bb4-4334-a2f1-df28fbf031ba",
   "metadata": {},
   "source": [
    "# Simple RAG (Retrieval-Augmented Generation) Implementation\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "\n",
    "1. **Document Ingestion**: How to download and process documents from object storage\n",
    "2. **Text Embeddings**: How to convert text into numerical vectors for semantic search\n",
    "3. **Vector Databases**: How to store and query embeddings efficiently using Milvus\n",
    "4. **Retrieval Process**: How to find relevant document chunks based on user queries\n",
    "5. **Augmented Generation**: How to combine retrieved context with LLMs for accurate responses\n",
    "\n",
    "## What is RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is a powerful technique that combines the strengths of:\n",
    "- **Information Retrieval**: Finding relevant documents from a knowledge base\n",
    "- **Generative AI**: Using Large Language Models to generate human-like responses\n",
    "\n",
    "### Why Use RAG?\n",
    "\n",
    "- **Up-to-date Information**: Access to current documents beyond the LLM's training data\n",
    "- **Reduced Hallucinations**: Grounding responses in factual, retrieved content\n",
    "- **Domain-Specific Knowledge**: Incorporate specialized documents and expertise\n",
    "- **Transparency**: See exactly which documents informed the response\n",
    "\n",
    "### The RAG Pipeline\n",
    "\n",
    "```\n",
    "Query ‚Üí Embedding ‚Üí Vector Search ‚Üí Context Retrieval ‚Üí LLM Generation ‚Üí Response\n",
    "```\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "This implementation demonstrates a simple but complete RAG system:\n",
    "\n",
    "1. **Document Storage**: MinIO/S3 object storage for source documents\n",
    "2. **Text Processing**: Docling for PDF parsing and chunking\n",
    "3. **Embeddings**: SentenceTransformers for converting text to vectors\n",
    "4. **Vector Database**: Milvus for storing and searching embeddings\n",
    "5. **LLM Integration**: Llama model for generating responses\n",
    "\n",
    "Let's dive into each component!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72373e99-c9a9-44be-b0a8-d572666c4a0b",
   "metadata": {},
   "source": [
    "## üîß Environment Setup and Dependencies\n",
    "\n",
    "Before we begin, let's install the required packages. This RAG implementation uses several key libraries:\n",
    "\n",
    "- **`docling`**: Advanced PDF parsing and document structure extraction\n",
    "- **`sentence-transformers`**: Pre-trained models for text embeddings\n",
    "- **`pymilvus`**: Python client for Milvus vector database\n",
    "- **`langchain-openai`**: LLM integration and prompt templating\n",
    "- **`boto3`**: AWS/MinIO S3 client for object storage\n",
    "- **`httpx`**: HTTP client for API calls\n",
    "- **`tqdm`**: Progress bars for better user experience\n",
    "\n",
    "> **Note**: The installation may take a few minutes as it downloads pre-trained models and dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd443db6-4eca-42c3-b63a-be3286249a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.11 environment at: /opt/app-root\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m10 packages\u001b[0m \u001b[2min 21ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install -r requirements.txt\n",
    "\n",
    "# !pip install openai==1.93.0      # Only for testing\n",
    "# ! pip install --upgrade docling openai torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08964f15-4c5b-48da-aaed-8d0bb817fc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Core Python libraries\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Object storage and cloud services\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "# Document processing and parsing\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling_core.transforms.chunker.hierarchical_chunker import HierarchicalChunker\n",
    "\n",
    "# Vector database\n",
    "from pymilvus import MilvusClient\n",
    "\n",
    "# LLM integration and prompt management\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# HTTP client for API calls\n",
    "import httpx\n",
    "\n",
    "# Progress tracking for user experience\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb50b9a4-9eae-4a1c-b8ef-d397ef7c0483",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration and Connection Setup\n",
    "\n",
    "This section configures the various services our RAG system depends on:\n",
    "\n",
    "### Object Storage Configuration\n",
    "- **MinIO/S3**: For storing and retrieving source documents\n",
    "- **Bucket and Object**: Specifies which document to process\n",
    "\n",
    "### Model and Database Configuration\n",
    "- **Embedding Model**: The SentenceTransformer model for text embeddings\n",
    "- **Vector Database**: Milvus connection details\n",
    "- **LLM Endpoint**: The inference server for generating responses\n",
    "\n",
    "> **Environment Variables**: These configurations are typically stored as environment variables for security and flexibility across different deployment environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f4d9cd0-05a5-4073-b76a-7ab829679400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Configuration loaded successfully!\n",
      "ü™£ Bucket containing files: rag-docs\n",
      "üìÅ Document to process: 2502.07835v1.pdf\n",
      "üîó Inference server: http://llama3-2-3b-predictor.llama-serving.svc.cluster.local:8080/v1\n",
      "üîó Inference model name: llama3-2-3b\n",
      "üóÉÔ∏è Vector database: http://milvus-service.milvus.svc.cluster.local:19530\n",
      "üß† Embedding model: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# ===== STORAGE CONFIGURATION =====\n",
    "# MinIO/S3 object storage settings - used for retrieving source documents\n",
    "endpoint = os.getenv(\"AWS_S3_ENDPOINT\")           # MinIO service DNS name (e.g. minio.minio.svc.cluster.local)\n",
    "access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")       # MinIO access key credentials\n",
    "secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")   # MinIO secret key credentials\n",
    "region = os.getenv(\"AWS_DEFAULT_REGION\")          # AWS region (dummy value for MinIO)\n",
    "bucket_name = os.getenv(\"AWS_S3_BUCKET\")          # Bucket containing our source documents\n",
    "object_key = \"2502.07835v1.pdf\"                   # Specific PDF document to process (research paper on AI code assessment)\n",
    "download_dir = \"downloads\"                        # Local directory for downloaded documents\n",
    "\n",
    "# ===== MODEL AND INFERENCE CONFIGURATION =====\n",
    "# LLM inference server - provides the generation capabilities for RAG responses\n",
    "# Default to a known default if the environment variable is not defined.\n",
    "inference_server_url = os.getenv(\"INFERENCE_SERVER_URL\", \"http://llama3-2-3b-predictor.llama-serving.svc.cluster.local:8080/v1\")\n",
    "inference_server_model_name = os.getenv(\"INFERENCE_SERVER_MODEL_NAME\", \"llama3-2-3b\")\n",
    "\n",
    "# Vector database configuration\n",
    "milvus_uri = \"http://milvus-service.milvus.svc.cluster.local:19530\"\n",
    "collection_name = \"my_rag_collection\"\n",
    "\n",
    "# Embedding model configuration\n",
    "embedding_model_name = \"all-MiniLM-L6-v2\"  # Lightweight, effective model for semantic similarity\n",
    "\n",
    "print(\"üîß Configuration loaded successfully!\")\n",
    "print(f\"ü™£ Bucket containing files: {bucket_name}\")\n",
    "print(f\"üìÅ Document to process: {object_key}\")\n",
    "print(f\"üîó Inference server: {inference_server_url}\")\n",
    "print(f\"üîó Inference model name: {inference_server_model_name}\")\n",
    "print(f\"üóÉÔ∏è Vector database: {milvus_uri}\")\n",
    "print(f\"üß† Embedding model: {embedding_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2ba0ab-7fd1-4a33-bea0-49eb2f846931",
   "metadata": {},
   "source": [
    "# üìÑ Document Ingestion\n",
    "\n",
    "## Understanding Document Ingestion in RAG\n",
    "\n",
    "Document ingestion is the first critical step in building a RAG system. It involves:\n",
    "\n",
    "1. **Document Retrieval**: Fetching documents from various sources (S3, file systems, databases)\n",
    "2. **Format Handling**: Supporting different document types (PDF, Word, text files)\n",
    "3. **Preprocessing**: Cleaning and preparing documents for further processing\n",
    "\n",
    "### Why Object Storage?\n",
    "\n",
    "Object storage (like S3 or MinIO) is ideal for RAG systems because:\n",
    "- **Scalability**: Can handle large volumes of documents\n",
    "- **Durability**: Built-in redundancy and backup capabilities  \n",
    "- **Accessibility**: Easy to integrate with various applications\n",
    "- **Cost-Effective**: Pay-per-use pricing model\n",
    "\n",
    "### The Document We're Processing\n",
    "\n",
    "In this example, we're working with a research paper titled \"ICE-Score: Instructional Code Evaluation through Large Language Models\". This paper discusses methods for evaluating AI-generated code quality - a perfect example of domain-specific knowledge that benefits from RAG.\n",
    "\n",
    "Let's download and prepare this document for processing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b8599ad-7f3c-4dbd-b0d6-64f73535ab03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Preparing to download document...\n",
      "   Source: rag-docs/2502.07835v1.pdf\n",
      "   Destination: downloads/2502.07835v1.pdf\n",
      "üîÑ Downloading document for processing...\n",
      "‚úÖ Successfully downloaded '2502.07835v1.pdf' to 'downloads/2502.07835v1.pdf'\n",
      "üìä File size: 1,870,265 bytes (1.8 MB)\n"
     ]
    }
   ],
   "source": [
    "# ===== INITIALIZE S3/MINIO CLIENT =====\n",
    "# Create boto3 client configured for MinIO (S3-compatible object storage)\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=f\"http://{endpoint}\",        # MinIO endpoint URL\n",
    "    aws_access_key_id=access_key,             # Authentication credentials\n",
    "    aws_secret_access_key=secret_key,\n",
    "    region_name=region,                       # Required by boto3, but not used by MinIO\n",
    "    config=Config(signature_version=\"s3v4\"),  # S3 signature version for authentication\n",
    ")\n",
    "\n",
    "# ===== PREPARE LOCAL STORAGE =====\n",
    "# Create local directory to store downloaded documents\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "local_path = os.path.join(download_dir, object_key)\n",
    "\n",
    "print(f\"üì• Preparing to download document...\")\n",
    "print(f\"   Source: {bucket_name}/{object_key}\")\n",
    "print(f\"   Destination: {local_path}\")\n",
    "\n",
    "# ===== DOWNLOAD DOCUMENT =====\n",
    "# Download the PDF document from object storage with proper error handling\n",
    "try:\n",
    "    print(f\"üîÑ Downloading document for processing...\")\n",
    "    s3.download_file(bucket_name, object_key, local_path)\n",
    "    print(f\"‚úÖ Successfully downloaded '{object_key}' to '{local_path}'\")\n",
    "    \n",
    "    # Verify the file was downloaded and get its size\n",
    "    file_size = os.path.getsize(local_path)\n",
    "    print(f\"üìä File size: {file_size:,} bytes ({file_size/1024/1024:.1f} MB)\")\n",
    "    \n",
    "except s3.exceptions.NoSuchKey:\n",
    "    print(f\"‚ùå ERROR: File '{object_key}' not found in bucket '{bucket_name}'\")\n",
    "    print(\"   Please check the bucket name and object key are correct.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: Failed to download file: {e}\")\n",
    "    print(\"   Please check your MinIO/S3 configuration and network connectivity.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce91f14c-7afc-4faa-8f18-f7e31a768804",
   "metadata": {},
   "source": [
    "## Embedding the text\n",
    "As we saw in the previous activity, whenever we store our objects in a vector database we need to convert them to a vector. To do that we need an embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cefa6e0-e615-429b-a25c-3a0e882a457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentenceTransformer for generating text embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\"\"\"\n",
    "Text Embedding Module  \n",
    "This module initialises a SentenceTransformer model using the ‚Äòall-MiniLM-L6-v2‚Äô embedding model and provides a function to generate text embeddings. (M.S. 0.98)\n",
    "\n",
    "Global Variables:\n",
    "    embedding_model (str): Name of the Hugging Face embedding model to load. (M.S. 0.98)\n",
    "    model (SentenceTransformer): Instance of SentenceTransformer initialised with the specified embedding model. (M.S. 0.98)\n",
    "\n",
    "Functions:\n",
    "    emb_text(text: str) -> list[float]:\n",
    "        Encode the input text and return its embedding vector as a list of floats. (M.S. 0.98)\n",
    "\"\"\"\n",
    "embedding_model=\"all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(embedding_model)\n",
    "\n",
    "def emb_text(text: str) -> list[float]:\n",
    "    return model.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25fce4f-8811-41c3-a93f-bc5d963708cc",
   "metadata": {},
   "source": [
    "### Test the embedding is working and also extract the dimensions\n",
    "This next code not only tests the embedding is working, but also determines the dimensions that the embedding model generates. We need that number for when we define the vector database schema later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c942a6e-18e7-4a1b-983d-c9bfbd32eb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing embedding function with sample text...\n",
      "üìä EMBEDDING ANALYSIS:\n",
      "   ‚Ä¢ Input text: 'This is a test sentence to demonstrate text embeddings.'\n",
      "   ‚Ä¢ Embedding dimensions: 384\n",
      "   ‚Ä¢ Data type: <class 'numpy.ndarray'>\n",
      "   ‚Ä¢ Sample values: [-0.00573698  0.00202521  0.07564172  0.0383721   0.02895643  0.0611613\n",
      " -0.01208316 -0.01273861  0.02499382 -0.04949658]\n",
      "   ‚Ä¢ Value range: [-0.1730, 0.1419]\n"
     ]
    }
   ],
   "source": [
    "# ===== EMBEDDING DIMENSION EXPLORATION =====\n",
    "# Test our embedding function to understand the output format and dimensions\n",
    "# This information is crucial for configuring the vector database schema\n",
    "\n",
    "print(\"üß™ Testing embedding function with sample text...\")\n",
    "test_text = \"This is a test sentence to demonstrate text embeddings.\"\n",
    "test_embedding = emb_text(test_text)\n",
    "embedding_dim = len(test_embedding)\n",
    "\n",
    "print(f\"üìä EMBEDDING ANALYSIS:\")\n",
    "print(f\"   ‚Ä¢ Input text: '{test_text}'\")\n",
    "print(f\"   ‚Ä¢ Embedding dimensions: {embedding_dim}\")\n",
    "print(f\"   ‚Ä¢ Data type: {type(test_embedding)}\")\n",
    "print(f\"   ‚Ä¢ Sample values: {test_embedding[:10]}\")\n",
    "print(f\"   ‚Ä¢ Value range: [{min(test_embedding):.4f}, {max(test_embedding):.4f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32670b30-629f-4563-b44f-e71d8b04aaff",
   "metadata": {},
   "source": [
    "# üìÑ Docling: Preparing Text for RAG Systems\n",
    "\n",
    "**Docling** is used to prepare documents for Retrieval-Augmented Generation (RAG) systems.\n",
    "\n",
    "Text documents such as PDFs, HTML pages, or plain text need to be **converted into chunks** that can be embedded and stored in a vector database. Docling simplifies this process by extracting, cleaning, and chunking content into well-structured semantic units.\n",
    "\n",
    "For this lab, Docling will be used to prepare documents before they are embedded and indexed in Milvus.\n",
    "\n",
    "---\n",
    "\n",
    "## What is Docling?\n",
    "\n",
    "**Docling** is a Python library designed for **document preparation** in GenAI pipelines. Its role is to transform unstructured content into structured, embeddable units, typically for use in:\n",
    "\n",
    "- **RAG applications**: Extracting retrievable units from long documents\n",
    "- **Semantic search pipelines**: Chunking content into searchable segments\n",
    "- **LLM input pipelines**: Creating well-scoped context windows\n",
    "\n",
    "Docling is particularly useful for converting complex documents like PDFs into consistent formats that can be embedded by LLMs.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Use Docling?\n",
    "\n",
    "**Docling** provides:\n",
    "\n",
    "- **Multi-format support**: PDF, DOCX, TXT, Markdown, HTML\n",
    "- **Intelligent chunking**: Preserves context while segmenting content\n",
    "- **Metadata extraction**: Title, headers, sections, and more\n",
    "- **Customisable workflows**: Fine-tune chunk size, overlap, and cleaning\n",
    "- **Streamlined RAG integration**: Designed to fit directly into vector pipelines\n",
    "\n",
    "---\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Chunking\n",
    "- Breaks large documents into smaller parts for efficient retrieval.\n",
    "- Can be based on token count, sentence boundaries, or structure (e.g. headings).\n",
    "- Prevents LLMs from exceeding context window limits.\n",
    "\n",
    "### Overlap\n",
    "- Ensures context is preserved across chunks.\n",
    "- Helpful in maintaining continuity of thought or narrative.\n",
    "\n",
    "### Metadata\n",
    "- Docling can attach metadata (e.g. file name, page number, section header) to each chunk.\n",
    "- Useful for traceability and debugging in RAG outputs.\n",
    "\n",
    "---\n",
    "\n",
    "## Typical Docling Workflow\n",
    "\n",
    "1. **Load the document**\n",
    "   - From local files or URLs\n",
    "\n",
    "2. **Parse and clean**\n",
    "   - Normalise spacing, remove boilerplate, handle special characters\n",
    "\n",
    "3. **Chunk**\n",
    "   - Create segments that fit within model context limits\n",
    "\n",
    "4. **Enrich**\n",
    "   - Add metadata such as section titles or page numbers\n",
    "\n",
    "5. **Output**\n",
    "   - Return a list of chunk objects ready for embedding\n",
    "\n",
    "---\n",
    "\n",
    "Let‚Äôs use Docling to transform raw documents into structured, retrievable content!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45716a2e-b56e-41db-86f3-40f0e720c781",
   "metadata": {},
   "source": [
    "### Verify the documents have been downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71546d8a-f032-4067-a609-c528194ab033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ INFO: Found document at: /opt/app-root/src/rhoai-roadshow-v2/docs/2-rag/notebook/downloads/2502.07835v1.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import project_root\n",
    "\n",
    "# Assemble a complete path to the file so the document import can properly and reliably always find the document.\n",
    "doc_source = project_root() / local_path\n",
    "\n",
    "if not doc_source.is_file():\n",
    "    raise FileNotFoundError(f\"{doc_source} does not exist.\")\n",
    "\n",
    "print(f\"üü¢ INFO: Found document at: {doc_source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1eb104d7-caf6-45d7-9b22-cfeb6a483d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parse and chunk a PDF using Docling v2.x\n",
    "\"\"\"\n",
    "doc = DocumentConverter().convert(source=doc_source).document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51b3591c-65eb-4556-a71b-0476c9b516b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ INFO: {1: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=1), 2: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=2), 3: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=3), 4: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=4), 5: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=5), 6: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=6), 7: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=7), 8: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=8), 9: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=9), 10: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=10), 11: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=11), 12: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=12), 13: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=13)}\n"
     ]
    }
   ],
   "source": [
    "print(f\"üü¢ INFO: {doc.pages}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7656184-67dd-41ce-83a9-3c3560337049",
   "metadata": {},
   "source": [
    "# Connect to Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bab9aa5-b709-4ff6-98e5-8aebf7759585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Connecting to Milvus vector database...\n",
      "‚úÖ Successfully connected to Milvus!\n",
      "   Endpoint: http://milvus-service.milvus.svc.cluster.local:19530\n",
      "   Existing collections: []\n",
      "   Target collection: my_rag_collection\n"
     ]
    }
   ],
   "source": [
    "# ===== MILVUS CONNECTION SETUP =====\n",
    "# Connect to the Milvus vector database instance\n",
    "print(\"üîó Connecting to Milvus vector database...\")\n",
    "\n",
    "# Initialize Milvus client with connection parameters\n",
    "milvus_client = MilvusClient(\n",
    "    uri=milvus_uri,                    # Database server endpoint\n",
    "    db_name=\"default\"                  # Database name (like schema in SQL)\n",
    ")\n",
    "\n",
    "# Test the connection\n",
    "try:\n",
    "    # List existing collections to verify connectivity\n",
    "    collections = milvus_client.list_collections()\n",
    "    print(f\"‚úÖ Successfully connected to Milvus!\")\n",
    "    print(f\"   Endpoint: {milvus_uri}\")\n",
    "    print(f\"   Existing collections: {collections}\")\n",
    "    print(f\"   Target collection: {collection_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to connect to Milvus: {e}\")\n",
    "    print(\"   Please check that Milvus is running and accessible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07c126b9-5d76-46ee-8a11-5e16336fdf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Preparing vector collection...\n",
      "   Collection 'my_rag_collection' does not exist - ready to create a new one\n"
     ]
    }
   ],
   "source": [
    "# ===== COLLECTION MANAGEMENT =====\n",
    "# Check if our collection already exists and clean it up if necessary\n",
    "print(\"üßπ Preparing vector collection...\")\n",
    "\n",
    "if milvus_client.has_collection(collection_name):\n",
    "    print(f\"   Collection '{collection_name}' already exists - removing it for a fresh start\")\n",
    "    milvus_client.drop_collection(collection_name)\n",
    "    print(\"   ‚úÖ Old collection removed\")\n",
    "else:\n",
    "    print(f\"   Collection '{collection_name}' does not exist - ready to create a new one\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c527d363-59fb-4e09-a7c1-7960d2b276b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è Creating vector collection...\n",
      "‚úÖ Collection 'my_rag_collection' created successfully!\n",
      "   Dimensions: 384\n",
      "   Metric Type: Inner Product (IP)\n",
      "   Consistency: Strong\n",
      "   Current collections: ['my_rag_collection']\n",
      "   Collection schema: {'collection_name': 'my_rag_collection', 'auto_id': False, 'num_shards': 1, 'description': '', 'fields': [{'field_id': 100, 'name': 'id', 'description': '', 'type': <DataType.INT64: 5>, 'params': {}, 'is_primary': True}, {'field_id': 101, 'name': 'vector', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 384}}], 'functions': [], 'aliases': [], 'collection_id': 459671240120884259, 'consistency_level': 0, 'properties': {}, 'num_partitions': 1, 'enable_dynamic_field': True}\n"
     ]
    }
   ],
   "source": [
    "# ===== COLLECTION CREATION =====\n",
    "# Create a new collection with the appropriate schema for our embeddings\n",
    "print(\"üèóÔ∏è Creating vector collection...\")\n",
    "\n",
    "milvus_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    dimension=embedding_dim,                # Must match our embedding model's output (384)\n",
    "    metric_type=\"IP\",                       # Inner Product - good for normalized embeddings\n",
    "    consistency_level=\"Strong\",             # Guarantees data consistency for this demo\n",
    "    # Other options: \"Session\", \"Bounded\", \"Eventually\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Collection '{collection_name}' created successfully!\")\n",
    "print(f\"   Dimensions: {embedding_dim}\")\n",
    "print(f\"   Metric Type: Inner Product (IP)\")\n",
    "print(f\"   Consistency: Strong\")\n",
    "\n",
    "# Verify the collection was created\n",
    "collections = milvus_client.list_collections()\n",
    "print(f\"   Current collections: {collections}\")\n",
    "\n",
    "# Get detailed information about our collection\n",
    "collection_info = milvus_client.describe_collection(collection_name)\n",
    "print(f\"   Collection schema: {collection_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "663ab01b-5591-413c-9b58-c756bc412602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Processing document with Docling...\n",
      "   Converting PDF: /opt/app-root/src/rhoai-roadshow-v2/docs/2-rag/notebook/downloads/2502.07835v1.pdf\n",
      "üìä DOCUMENT ANALYSIS:\n",
      "   ‚Ä¢ Total pages: 13\n",
      "   ‚Ä¢ Document type: <class 'docling_core.types.doc.document.DoclingDocument'>\n",
      "   ‚Ä¢ Processing strategy: Structure-aware parsing\n",
      "\n",
      "üî™ Chunking document into smaller pieces...\n",
      "   ‚Ä¢ Strategy: Hierarchical chunking\n",
      "   ‚Ä¢ Benefits: Preserves document structure and context\n",
      "üìà CHUNKING RESULTS:\n",
      "   ‚Ä¢ Total chunks created: 70\n",
      "   ‚Ä¢ Average chunk length: 326 characters\n",
      "   ‚Ä¢ Shortest chunk: 6 characters\n",
      "   ‚Ä¢ Longest chunk: 1545 characters\n",
      "\n",
      "üìù SAMPLE CHUNK (first 200 characters):\n",
      "   \"ahilanp@gmail.com...\"\n",
      "\n",
      "‚úÖ Document processing complete! Ready for embedding generation.\n"
     ]
    }
   ],
   "source": [
    "# ===== DOCUMENT PROCESSING AND CHUNKING =====\n",
    "# Now let's process our PDF document and break it into searchable chunks\n",
    "print(\"üìÑ Processing document with Docling...\")\n",
    "\n",
    "# Initialize document converter and chunker\n",
    "converter = DocumentConverter()\n",
    "chunker = HierarchicalChunker()\n",
    "\n",
    "# Convert the PDF to a structured document object\n",
    "print(f\"   Converting PDF: {doc_source}\")\n",
    "doc = converter.convert(source=doc_source).document\n",
    "\n",
    "# Analyze document structure\n",
    "print(f\"üìä DOCUMENT ANALYSIS:\")\n",
    "print(f\"   ‚Ä¢ Total pages: {len(doc.pages)}\")\n",
    "print(f\"   ‚Ä¢ Document type: {type(doc)}\")\n",
    "\n",
    "# Document metadata would be available here if the document contained it\n",
    "print(f\"   ‚Ä¢ Processing strategy: Structure-aware parsing\")\n",
    "\n",
    "# Perform hierarchical chunking\n",
    "print(f\"\\nüî™ Chunking document into smaller pieces...\")\n",
    "print(f\"   ‚Ä¢ Strategy: Hierarchical chunking\")\n",
    "print(f\"   ‚Ä¢ Benefits: Preserves document structure and context\")\n",
    "\n",
    "# Extract text chunks from the document\n",
    "texts = [chunk.text for chunk in chunker.chunk(doc)]\n",
    "\n",
    "print(f\"üìà CHUNKING RESULTS:\")\n",
    "print(f\"   ‚Ä¢ Total chunks created: {len(texts)}\")\n",
    "print(f\"   ‚Ä¢ Average chunk length: {sum(len(text) for text in texts) / len(texts):.0f} characters\")\n",
    "print(f\"   ‚Ä¢ Shortest chunk: {min(len(text) for text in texts)} characters\")\n",
    "print(f\"   ‚Ä¢ Longest chunk: {max(len(text) for text in texts)} characters\")\n",
    "\n",
    "# Show a sample chunk\n",
    "print(f\"\\nüìù SAMPLE CHUNK (first 200 characters):\")\n",
    "print(f\"   \\\"{texts[0][:200]}...\\\"\")\n",
    "\n",
    "print(f\"\\n‚úÖ Document processing complete! Ready for embedding generation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dfd8b8-34b9-4377-98aa-fe0b7fb8011a",
   "metadata": {},
   "source": [
    "# üíæ Vector Storage and Search\n",
    "\n",
    "## Understanding Vector Storage\n",
    "\n",
    "Now that we have processed our document into chunks, we need to:\n",
    "\n",
    "1. **Convert each chunk to embeddings**: Transform text into numerical vectors\n",
    "2. **Store in vector database**: Save embeddings with metadata for efficient retrieval\n",
    "3. **Index for search**: Prepare the database for fast similarity queries\n",
    "\n",
    "## The Embedding Process\n",
    "\n",
    "For each text chunk, we will:\n",
    "- **Generate embeddings** using our SentenceTransformer model\n",
    "- **Store the vector** along with the original text in Milvus\n",
    "- **Create an index** for fast similarity search\n",
    "\n",
    "## Why This Approach Works\n",
    "\n",
    "- **Semantic Understanding**: Vector representations capture meaning, not just keywords\n",
    "- **Scalability**: Vector databases handle millions of embeddings efficiently\n",
    "- **Fast Retrieval**: Approximate nearest neighbor search provides quick results\n",
    "- **Flexibility**: Easy to update, add, or remove documents\n",
    "\n",
    "Let's embed our document chunks and store them in the vector database!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7da3aeba-3526-48e6-a2ff-c4d00aa2c72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Generating embeddings for all document chunks...\n",
      "   Processing 70 chunks with all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üßÆ Embedding chunks: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 70/70 [00:00<00:00, 192.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä EMBEDDING STATISTICS:\n",
      "   ‚Ä¢ Total chunks embedded: 70\n",
      "   ‚Ä¢ Total words processed: 3,313\n",
      "   ‚Ä¢ Average words per chunk: 47.3\n",
      "   ‚Ä¢ Embedding dimensions: 384\n",
      "   ‚Ä¢ Memory usage: ~0.1 MB\n",
      "\n",
      "üíæ Storing embeddings in Milvus vector database...\n",
      "   Collection: my_rag_collection\n",
      "   Records to insert: 70\n",
      "‚úÖ STORAGE COMPLETE!\n",
      "   Insert result: {'insert_count': 70, 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]}\n",
      "   Total vectors stored: 70\n",
      "   Storage cost: 0\n",
      "   Collection stats: {'row_count': 0}\n",
      "\n",
      "üîç Vector database is ready for similarity search!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== EMBEDDING GENERATION AND STORAGE =====\n",
    "# Convert all text chunks to embeddings and store them in the vector database\n",
    "print(\"üîÑ Generating embeddings for all document chunks...\")\n",
    "print(f\"   Processing {len(texts)} chunks with {embedding_model_name}\")\n",
    "\n",
    "# Prepare data structure for batch insertion\n",
    "data = []\n",
    "total_tokens = 0\n",
    "\n",
    "# Process each chunk: generate embedding and prepare for storage\n",
    "for i, chunk in enumerate(tqdm(texts, desc=\"üßÆ Embedding chunks\")):\n",
    "    # Generate embedding for this chunk\n",
    "    embedding = emb_text(chunk)\n",
    "    \n",
    "    # Prepare data record for Milvus\n",
    "    # Each record contains: unique ID, vector embedding, original text\n",
    "    data.append({\n",
    "        \"id\": i,                    # Unique identifier for this chunk\n",
    "        \"vector\": embedding,        # The embedding vector (384 dimensions)\n",
    "        \"text\": chunk              # Original text for retrieval and display\n",
    "    })\n",
    "    \n",
    "    # Track statistics\n",
    "    total_tokens += len(chunk.split())\n",
    "\n",
    "# Display embedding statistics\n",
    "print(f\"\\nüìä EMBEDDING STATISTICS:\")\n",
    "print(f\"   ‚Ä¢ Total chunks embedded: {len(data)}\")\n",
    "print(f\"   ‚Ä¢ Total words processed: {total_tokens:,}\")\n",
    "print(f\"   ‚Ä¢ Average words per chunk: {total_tokens/len(data):.1f}\")\n",
    "print(f\"   ‚Ä¢ Embedding dimensions: {len(data[0]['vector'])}\")\n",
    "print(f\"   ‚Ä¢ Memory usage: ~{len(data) * len(data[0]['vector']) * 4 / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# ===== BATCH INSERT TO MILVUS =====\n",
    "print(f\"\\nüíæ Storing embeddings in Milvus vector database...\")\n",
    "print(f\"   Collection: {collection_name}\")\n",
    "print(f\"   Records to insert: {len(data)}\")\n",
    "\n",
    "# Perform batch insert for efficiency\n",
    "insert_result = milvus_client.insert(collection_name=collection_name, data=data)\n",
    "\n",
    "print(f\"‚úÖ STORAGE COMPLETE!\")\n",
    "print(f\"   Insert result: {insert_result}\")\n",
    "print(f\"   Total vectors stored: {insert_result['insert_count']}\")\n",
    "print(f\"   Storage cost: {insert_result['cost']}\")\n",
    "\n",
    "# Verify the data was inserted correctly\n",
    "collection_stats = milvus_client.get_collection_stats(collection_name)\n",
    "print(f\"   Collection stats: {collection_stats}\")\n",
    "\n",
    "print(f\"\\nüîç Vector database is ready for similarity search!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24873e3-fe35-4f22-80a6-426ffc890765",
   "metadata": {},
   "source": [
    "# üìä Visualizing Vector Embeddings (Optional)\n",
    "\n",
    "## Understanding Vector Spaces\n",
    "\n",
    "Embeddings exist in high-dimensional space (384 dimensions in our case), which is difficult to visualize directly. However, we can use dimensionality reduction techniques to project these vectors into 2D or 3D space for visualization.\n",
    "\n",
    "## Tools for Visualization\n",
    "\n",
    "### TensorFlow Projector\n",
    "- **URL**: https://projector.tensorflow.org/\n",
    "- **Purpose**: Interactive visualization of high-dimensional embeddings\n",
    "- **Features**: \n",
    "  - PCA and t-SNE dimensionality reduction\n",
    "  - Color-coding and clustering\n",
    "  - Interactive exploration of vector neighborhoods\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Dimensionality Reduction**: Algorithms like PCA or t-SNE compress 384D vectors to 2D/3D\n",
    "2. **Semantic Clustering**: Similar concepts appear close together in the visualization\n",
    "3. **Interactive Exploration**: Click on points to see the original text and find similar chunks\n",
    "\n",
    "## What You Would See\n",
    "\n",
    "- **Document Clusters**: Related sections of the paper grouped together\n",
    "- **Concept Boundaries**: Clear separation between different topics\n",
    "- **Similarity Relationships**: Semantic connections between text chunks\n",
    "\n",
    "## Educational Value\n",
    "\n",
    "Visualizing embeddings helps you understand:\n",
    "- How semantic similarity translates to geometric proximity\n",
    "- Why vector search is effective for finding related content\n",
    "- The relationship between embedding quality and retrieval performance\n",
    "\n",
    "> **Note**: While visualization is helpful for understanding, it's optional for the RAG system functionality. The vector database performs searches directly in the high-dimensional space without needing to reduce dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054fbcd4-be4b-4a35-b8e3-9f33ea5d0d3d",
   "metadata": {},
   "source": [
    "# üîç Query-Time Retrieval\n",
    "\n",
    "## How RAG Retrieval Works\n",
    "\n",
    "When a user asks a question, the RAG system performs the following steps:\n",
    "\n",
    "1. **Query Embedding**: Convert the user's question into the same vector space as our documents\n",
    "2. **Similarity Search**: Find the most similar document chunks using vector distance\n",
    "3. **Ranking**: Sort results by relevance score (similarity distance)\n",
    "4. **Selection**: Choose the top-k most relevant chunks for context\n",
    "\n",
    "## Vector Search Process\n",
    "\n",
    "### Step 1: Query Embedding\n",
    "- Use the **same embedding model** that was used for document chunks\n",
    "- This ensures query and document vectors are in the same semantic space\n",
    "\n",
    "### Step 2: Distance Calculation\n",
    "- **Inner Product (IP)**: Our chosen metric - higher values mean more similar\n",
    "- **Cosine Similarity**: Measures angle between vectors (normalized IP)\n",
    "- **Euclidean Distance**: Straight-line distance in vector space\n",
    "\n",
    "### Step 3: Approximate Nearest Neighbor (ANN)\n",
    "- **Speed vs Accuracy**: ANN provides fast search with minimal accuracy loss\n",
    "- **Indexing**: Milvus creates indexes for efficient search across millions of vectors\n",
    "- **Scalability**: Can handle large document collections in real-time\n",
    "\n",
    "## Retrieval Parameters\n",
    "\n",
    "- **Limit**: Number of top results to return (we'll use 3)\n",
    "- **Search Params**: Configuration for the search algorithm\n",
    "- **Output Fields**: Which metadata to return with results (we want the original text)\n",
    "\n",
    "Let's test our retrieval system with a sample question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d1fbadd-29f1-4192-95f4-8b81232fa0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì USER QUESTION:\n",
      "   \"What are the challenges of assessing the quality of AI-generated code? What are some strategies for doing this?\"\n",
      "\n",
      "üéØ Why this question tests RAG effectively:\n",
      "   ‚Ä¢ Domain-specific: Related to AI code evaluation\n",
      "   ‚Ä¢ Multi-faceted: Asks for both challenges AND strategies\n",
      "   ‚Ä¢ Synthesis required: Needs information from multiple sources\n",
      "   ‚Ä¢ Context-dependent: Benefits from specific document knowledge\n"
     ]
    }
   ],
   "source": [
    "# ===== DEFINE USER QUERY =====\n",
    "# This is the question we want to answer using our RAG system\n",
    "question = (\n",
    "    \"What are the challenges of assessing the quality of AI-generated code? What are some strategies for doing this?\"\n",
    ")\n",
    "\n",
    "print(\"‚ùì USER QUESTION:\")\n",
    "print(f\"   \\\"{question}\\\"\")\n",
    "\n",
    "# This question is perfect for testing our RAG system because:\n",
    "# 1. It's directly related to our document (AI code evaluation)\n",
    "# 2. It has two parts - challenges AND strategies\n",
    "# 3. It requires synthesis of information from multiple sections\n",
    "# 4. It's the kind of question that benefits from retrieved context\n",
    "\n",
    "print(f\"\\nüéØ Why this question tests RAG effectively:\")\n",
    "print(f\"   ‚Ä¢ Domain-specific: Related to AI code evaluation\")\n",
    "print(f\"   ‚Ä¢ Multi-faceted: Asks for both challenges AND strategies\")\n",
    "print(f\"   ‚Ä¢ Synthesis required: Needs information from multiple sources\")\n",
    "print(f\"   ‚Ä¢ Context-dependent: Benefits from specific document knowledge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd66032c-3d2b-4c87-936c-31031f4d84a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Converting question to embedding...\n",
      "   Question embedding shape: 384 dimensions\n",
      "   Embedding sample: [-0.08172768 -0.02636518 -0.04366045  0.04490916  0.0063496 ]\n",
      "\n",
      "üîç Searching for similar document chunks...\n",
      "   Collection: my_rag_collection\n",
      "   Search method: Vector similarity using Inner Product\n",
      "   Top results to return: 3\n",
      "\n",
      "üìä SEARCH RESULTS ANALYSIS:\n",
      "   ‚Ä¢ Total matches found: 3\n",
      "   ‚Ä¢ Search completed successfully!\n",
      "\n",
      "üîç RESULT #1:\n",
      "   ‚Ä¢ Similarity score: 0.7006\n",
      "   ‚Ä¢ Chunk ID: 2\n",
      "   ‚Ä¢ Text preview: \"The rise of Large Language Models (LLMs) in software engineering, particularly in code generation, h...\"\n",
      "\n",
      "üîç RESULT #2:\n",
      "   ‚Ä¢ Similarity score: 0.6455\n",
      "   ‚Ä¢ Chunk ID: 10\n",
      "   ‚Ä¢ Text preview: \"The SBC score, along with the reverse-generated requirements, provides actionable insights for devel...\"\n",
      "\n",
      "üîç RESULT #3:\n",
      "   ‚Ä¢ Similarity score: 0.6278\n",
      "   ‚Ä¢ Chunk ID: 4\n",
      "   ‚Ä¢ Text preview: \"AI-powered code assistants, leveraging the power of Large Language Models (LLMs), are becoming a foc...\"\n",
      "\n",
      "‚úÖ Retrieved 3 relevant chunks for context!\n"
     ]
    }
   ],
   "source": [
    "# ===== PERFORM VECTOR SEARCH =====\n",
    "# Step 1: Convert the question to an embedding vector\n",
    "print(\"üîÑ Converting question to embedding...\")\n",
    "question_embedding = emb_text(question)\n",
    "\n",
    "print(f\"   Question embedding shape: {len(question_embedding)} dimensions\")\n",
    "print(f\"   Embedding sample: {question_embedding[:5]}\")\n",
    "\n",
    "# Step 2: Search for similar vectors in the database\n",
    "print(f\"\\nüîç Searching for similar document chunks...\")\n",
    "print(f\"   Collection: {collection_name}\")\n",
    "print(f\"   Search method: Vector similarity using Inner Product\")\n",
    "print(f\"   Top results to return: 3\")\n",
    "\n",
    "# Perform the vector search\n",
    "search_res = milvus_client.search(\n",
    "    collection_name=collection_name,\n",
    "    data=[question_embedding],                    # Query vector (must be in a list)\n",
    "    limit=3,                                      # Number of top results to return\n",
    "    search_params={\"metric_type\": \"IP\", \"params\": {}},  # Inner Product distance\n",
    "    output_fields=[\"text\"],                       # Return the original text with results\n",
    ")\n",
    "\n",
    "# Analyze search results\n",
    "print(f\"\\nüìä SEARCH RESULTS ANALYSIS:\")\n",
    "print(f\"   ‚Ä¢ Total matches found: {len(search_res[0])}\")\n",
    "print(f\"   ‚Ä¢ Search completed successfully!\")\n",
    "\n",
    "# Display each result with its similarity score\n",
    "for i, result in enumerate(search_res[0]):\n",
    "    score = result[\"distance\"]\n",
    "    chunk_id = result[\"id\"]\n",
    "    text_preview = result[\"entity\"][\"text\"][:100] + \"...\" if len(result[\"entity\"][\"text\"]) > 100 else result[\"entity\"][\"text\"]\n",
    "    \n",
    "    print(f\"\\nüîç RESULT #{i+1}:\")\n",
    "    print(f\"   ‚Ä¢ Similarity score: {score:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Chunk ID: {chunk_id}\")\n",
    "    print(f\"   ‚Ä¢ Text preview: \\\"{text_preview}\\\"\")\n",
    "\n",
    "print(f\"\\n‚úÖ Retrieved {len(search_res[0])} relevant chunks for context!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e78eac4-fd36-4321-a629-f81f4120e4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Processing retrieved chunks for LLM context...\n",
      "\n",
      "üìã RETRIEVED CHUNKS (Raw Format):\n",
      "\n",
      "--- CHUNK 1 (Similarity: 0.7006) ---\n",
      "The rise of Large Language Models (LLMs) in software engineering, particularly in code generation, has garnered significant attention. However, assessing the quality of AI-generated code remains a challenge due to the inherent complexity of programming tasks and the lack of robust evaluation metrics that align well with human judgment. Traditional token-based metrics such as BLEU and ROUGE, while commonly used in natural language processing, exhibit weak correlations with human assessments in code intelligence and verification tasks. Furthermore, these metrics are primarily research focused and are not designed for seamless integration into the software development lifecycle, limiting their practical utility for developers seeking to improve code quality and security.\n",
      "--- END CHUNK 1 ---\n",
      "\n",
      "--- CHUNK 2 (Similarity: 0.6455) ---\n",
      "The SBC score, along with the reverse-generated requirements, provides actionable insights for developers, helping them assess AI-generated code without requiring extensive reference implementations. Unlike prior evaluation methods, this approach inherently addresses the challenges of syntactic variations and alternative solutions in generated code, as highlighted in recent studies [12]. By\n",
      "--- END CHUNK 2 ---\n",
      "\n",
      "--- CHUNK 3 (Similarity: 0.6278) ---\n",
      "AI-powered code assistants, leveraging the power of Large Language Models (LLMs), are becoming a focal point for enterprises, offering promising capabilities in automating code generation. However, evaluating the quality of LLM-generated code remains a complex challenge due to the intricacies of programming concepts and syntax, which differ significantly from natural language generation [1, 2].\n",
      "--- END CHUNK 3 ---\n",
      "\n",
      "üìä RETRIEVED CONTENT STATISTICS:\n",
      "   ‚Ä¢ Total chunks: 3\n",
      "   ‚Ä¢ Total characters: 1,568\n",
      "   ‚Ä¢ Total words: 211\n",
      "   ‚Ä¢ Average words per chunk: 70\n",
      "   ‚Ä¢ Similarity score range: 0.6278 - 0.7006\n",
      "\n",
      "üîç DEBUG: Raw JSON format of retrieved chunks:\n",
      "[\n",
      "  [\n",
      "    \"The rise of Large Language Models (LLMs) in software engineering, particularly in code generation, has garnered significant attention. However, assessing the quality of AI-generated code remains a challenge due to the inherent complexity of programming tasks and the lack of robust evaluation metrics that align well with human judgment. Traditional token-based metrics such as BLEU and ROUGE, while commonly used in natural language processing, exhibit weak correlations with human assessments in code intelligence and verification tasks. Furthermore, these metrics are primarily research focused and are not designed for seamless integration into the software development lifecycle, limiting their practical utility for developers seeking to improve code quality and security.\",\n",
      "    0.7005951404571533\n",
      "  ],\n",
      "  [\n",
      "    \"The SBC score, along with the reverse-generated requirements, provides actionable insights for developers, helping them assess AI-generated code without requiring extensive reference implementations. Unlike prior evaluation methods, this approach inherently addresses the challenges of syntactic variations and alternative solutions in generated code, as highlighted in recent studies [12]. By\",\n",
      "    0.6454936265945435\n",
      "  ],\n",
      "  [\n",
      "    \"AI-powered code assistants, leveraging the power of Large Language Models (LLMs), are becoming a focal point for enterprises, offering promising capabilities in automating code generation. However, evaluating the quality of LLM-generated code remains a complex challenge due to the intricacies of programming concepts and syntax, which differ significantly from natural language generation [1, 2].\",\n",
      "    0.6278239488601685\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# ===== PROCESS SEARCH RESULTS =====\n",
    "# Extract text and similarity scores from the search results for context preparation\n",
    "print(\"üìù Processing retrieved chunks for LLM context...\")\n",
    "\n",
    "# Create structured data with text and similarity scores\n",
    "retrieved_lines_with_distances = [\n",
    "    (res[\"entity\"][\"text\"], res[\"distance\"]) for res in search_res[0]\n",
    "]\n",
    "\n",
    "# Display the raw results in a structured format\n",
    "print(f\"\\nüìã RETRIEVED CHUNKS (Raw Format):\")\n",
    "for i, (text, distance) in enumerate(retrieved_lines_with_distances):\n",
    "    print(f\"\\n--- CHUNK {i+1} (Similarity: {distance:.4f}) ---\")\n",
    "    print(f\"{text}\")\n",
    "    print(f\"--- END CHUNK {i+1} ---\")\n",
    "\n",
    "# Statistics about retrieved content\n",
    "total_chars = sum(len(text) for text, _ in retrieved_lines_with_distances)\n",
    "total_words = sum(len(text.split()) for text, _ in retrieved_lines_with_distances)\n",
    "\n",
    "print(f\"\\nüìä RETRIEVED CONTENT STATISTICS:\")\n",
    "print(f\"   ‚Ä¢ Total chunks: {len(retrieved_lines_with_distances)}\")\n",
    "print(f\"   ‚Ä¢ Total characters: {total_chars:,}\")\n",
    "print(f\"   ‚Ä¢ Total words: {total_words:,}\")\n",
    "print(f\"   ‚Ä¢ Average words per chunk: {total_words/len(retrieved_lines_with_distances):.0f}\")\n",
    "print(f\"   ‚Ä¢ Similarity score range: {min(d for _, d in retrieved_lines_with_distances):.4f} - {max(d for _, d in retrieved_lines_with_distances):.4f}\")\n",
    "\n",
    "# For debugging purposes, also show the JSON format\n",
    "print(f\"\\nüîç DEBUG: Raw JSON format of retrieved chunks:\")\n",
    "print(json.dumps(retrieved_lines_with_distances, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8613c6a7-b781-4e48-a8af-41bb97b9f03c",
   "metadata": {},
   "source": [
    "# ü§ñ Augmented Generation\n",
    "\n",
    "## From Retrieval to Response\n",
    "\n",
    "Now comes the final step of RAG - using the retrieved context to generate a well-informed response. This process involves:\n",
    "\n",
    "1. **Context Preparation**: Combine retrieved chunks into a coherent context\n",
    "2. **Prompt Engineering**: Structure the prompt to include context and question\n",
    "3. **LLM Generation**: Use the language model to generate a response\n",
    "4. **Response Synthesis**: Produce a final answer based on the retrieved evidence\n",
    "\n",
    "## The Power of Context\n",
    "\n",
    "Without RAG, an LLM would answer based only on its training data, which might:\n",
    "- **Lack specific information** about our document\n",
    "- **Provide outdated information** if the model is older\n",
    "- **Generate hallucinations** without factual grounding\n",
    "\n",
    "With RAG, the LLM has access to:\n",
    "- **Relevant, specific content** from our document\n",
    "- **Current information** from the retrieved chunks\n",
    "- **Factual grounding** to reduce hallucinations\n",
    "\n",
    "## Prompt Engineering for RAG\n",
    "\n",
    "A well-designed RAG prompt includes:\n",
    "- **System instructions** that define the AI's role and constraints\n",
    "- **Retrieved context** that provides factual information\n",
    "- **User question** that specifies what to answer\n",
    "- **Response guidelines** that ensure appropriate formatting\n",
    "\n",
    "Let's see how this works in practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c45528a5-4cbf-4f88-a720-aeb614923d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Preparing context for LLM generation...\n",
      "üìä CONTEXT STATISTICS:\n",
      "   ‚Ä¢ Context length: 1,572 characters\n",
      "   ‚Ä¢ Context words: 211 words\n",
      "   ‚Ä¢ Number of chunks: 3\n",
      "\n",
      "üìù PREPARED CONTEXT (first 300 characters):\n",
      "   \"The rise of Large Language Models (LLMs) in software engineering, particularly in code generation, has garnered significant attention. However, assessing the quality of AI-generated code remains a challenge due to the inherent complexity of programming tasks and the lack of robust evaluation metrics...\"\n",
      "\n",
      "‚úÖ Context prepared successfully!\n",
      "   The LLM will use this context to generate an informed response.\n"
     ]
    }
   ],
   "source": [
    "# ===== CONTEXT PREPARATION =====\n",
    "# Combine the retrieved chunks into a single context string for the LLM\n",
    "print(\"üìã Preparing context for LLM generation...\")\n",
    "\n",
    "# Extract just the text (without similarity scores) and join them\n",
    "context_chunks = [text for text, _ in retrieved_lines_with_distances]\n",
    "context = \"\\n\\n\".join(context_chunks)  # Use double newlines for better separation\n",
    "\n",
    "print(f\"üìä CONTEXT STATISTICS:\")\n",
    "print(f\"   ‚Ä¢ Context length: {len(context):,} characters\")\n",
    "print(f\"   ‚Ä¢ Context words: {len(context.split()):,} words\")\n",
    "print(f\"   ‚Ä¢ Number of chunks: {len(context_chunks)}\")\n",
    "\n",
    "# Show the prepared context (truncated for readability)\n",
    "print(f\"\\nüìù PREPARED CONTEXT (first 300 characters):\")\n",
    "print(f\"   \\\"{context[:300]}...\\\"\")\n",
    "\n",
    "# This context will be included in the prompt to provide the LLM with\n",
    "# relevant information from our document to answer the user's question\n",
    "print(f\"\\n‚úÖ Context prepared successfully!\")\n",
    "print(f\"   The LLM will use this context to generate an informed response.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57421cbd-7cf8-42ad-a9c1-3b794d1e6134",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Prompt Engineering for RAG\n",
    "\n",
    "Effective prompt engineering is crucial for RAG success. Our prompts need to:\n",
    "\n",
    "### System Prompt Design\n",
    "- **Role Definition**: Clearly specify the AI's role and constraints\n",
    "- **Context Grounding**: Ensure responses are based only on provided context\n",
    "- **Honesty Enforcement**: Require admission when information is unavailable\n",
    "- **Quality Guidelines**: Set expectations for response structure and completeness\n",
    "\n",
    "### User Prompt Structure\n",
    "- **Context Section**: Present retrieved information clearly\n",
    "- **Question Section**: State the user's question explicitly\n",
    "- **Response Instructions**: Guide the format and style of the answer\n",
    "\n",
    "### Why This Matters\n",
    "- **Reduces Hallucinations**: Strict context adherence prevents made-up information\n",
    "- **Improves Relevance**: Clear instructions help focus on what's important\n",
    "- **Ensures Consistency**: Structured prompts lead to predictable response formats\n",
    "- **Enhances Quality**: Well-designed prompts improve response accuracy and usefulness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48a52e88-eeb0-4766-bfd2-c1c718f008ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "  \"You are an AI assistant that answers questions based solely on the provided context. \"\n",
    "  \"If the answer cannot be found in context, reply truthfully that you don‚Äôt know.\"\n",
    ")\n",
    "\n",
    "USER_PROMPT = (\n",
    "  \"Context:\\n\"\n",
    "  \"{context}\\n\"\n",
    "  \"Question:\\n\"\n",
    "  \"{question}\\n\"\n",
    "  \"Answer concisely:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ddcc05-f348-4d62-8466-a37059310575",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è LLM Integration and Response Generation\n",
    "\n",
    "## Language Model Setup\n",
    "\n",
    "Our RAG system uses a **Llama 3.2 3B model** that's been quantized for efficiency. Key configuration choices:\n",
    "\n",
    "### Model Configuration\n",
    "- **Temperature: 0**: Ensures deterministic, consistent responses\n",
    "- **Max Tokens: None**: Allows full-length responses without artificial cutoffs\n",
    "- **Retries: 2**: Handles temporary network or service issues\n",
    "- **SSL Verification: Disabled**: Required for internal service endpoints\n",
    "\n",
    "### Why Llama 3.2 3B?\n",
    "- **Efficiency**: Smaller model with good performance for focused tasks\n",
    "- **Quantization**: 8-bit quantization reduces memory usage while maintaining quality\n",
    "- **Instruction Following**: Fine-tuned to follow instructions and answer questions accurately\n",
    "- **Context Awareness**: Capable of understanding and using provided context effectively\n",
    "\n",
    "## The Generation Process\n",
    "\n",
    "1. **Prompt Construction**: Combine system instructions, context, and question\n",
    "2. **Model Invocation**: Send the complete prompt to the LLM\n",
    "3. **Response Generation**: Model generates answer based on context\n",
    "4. **Result Processing**: Extract and present the final response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3976092f-fd56-44f0-9822-d073acd5a245",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=inference_server_model_name,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=\"EMPTY\",  # if you prefer to pass api key in directly instaed of using env vars\n",
    "    base_url=inference_server_url,\n",
    "    http_client=httpx.Client(verify=False)    # Because we are using an internal API endpoint (service) we need to disable SSL certificate checking.\n",
    ")\n",
    "\n",
    "# Define system and human templates\n",
    "SYSTEM_PROMPT = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an AI assistant that answers questions based solely on the provided context. \"\n",
    "    \"If the answer cannot be found in context, reply truthfully that you don‚Äôt know.\"\n",
    ")\n",
    "\n",
    "HumanMessagePromptTemplate = HumanMessagePromptTemplate.from_template(\n",
    "    \"Context:\\n\"\n",
    "    \"{context}\\n\"\n",
    "    \"Question:\\n\"\n",
    "    \"{question}\\n\"\n",
    "    \"Answer concisely:\"\n",
    ")\n",
    "\n",
    "# Combine into a chat prompt\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [SYSTEM_PROMPT, HumanMessagePromptTemplate]\n",
    ")\n",
    "\n",
    "prompt = chat_prompt.format_prompt(context=context, question=question)\n",
    "\n",
    "ai_msg = llm.invoke(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93c9a430-0118-4faa-a657-672927bcdda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ RAG PIPELINE RESULTS\n",
      "==================================================\n",
      "\n",
      "‚ùì ORIGINAL QUESTION:\n",
      "   What are the challenges of assessing the quality of AI-generated code? What are some strategies for doing this?\n",
      "\n",
      "üìä PIPELINE STATISTICS:\n",
      "   ‚Ä¢ Document chunks retrieved: 3\n",
      "   ‚Ä¢ Context length: 1,572 characters\n",
      "   ‚Ä¢ Context words: 211\n",
      "   ‚Ä¢ Similarity scores: ['0.701', '0.645', '0.628']\n",
      "\n",
      "üìù RETRIEVED CONTEXT SUMMARY:\n",
      "   Chunk 1 (0.701): The rise of Large Language Models (LLMs) in software engineering, particularly i...\n",
      "   Chunk 2 (0.645): The SBC score, along with the reverse-generated requirements, provides actionabl...\n",
      "   Chunk 3 (0.628): AI-powered code assistants, leveraging the power of Large Language Models (LLMs)...\n"
     ]
    }
   ],
   "source": [
    "# ===== DISPLAY RESULTS =====\n",
    "# Show the complete RAG pipeline results for analysis and learning\n",
    "print(\"üéØ RAG PIPELINE RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\n‚ùì ORIGINAL QUESTION:\")\n",
    "print(f\"   {question}\")\n",
    "\n",
    "print(f\"\\nüìä PIPELINE STATISTICS:\")\n",
    "print(f\"   ‚Ä¢ Document chunks retrieved: {len(retrieved_lines_with_distances)}\")\n",
    "print(f\"   ‚Ä¢ Context length: {len(context):,} characters\")\n",
    "print(f\"   ‚Ä¢ Context words: {len(context.split()):,}\")\n",
    "print(f\"   ‚Ä¢ Similarity scores: {[f'{d:.3f}' for _, d in retrieved_lines_with_distances]}\")\n",
    "\n",
    "print(f\"\\nüìù RETRIEVED CONTEXT SUMMARY:\")\n",
    "for i, (text, score) in enumerate(retrieved_lines_with_distances):\n",
    "    print(f\"   Chunk {i+1} ({score:.3f}): {text[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6d6e266-fb32-4094-bd65-b9ea5ed05e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† CONTEXT PROVIDED TO LLM:\n",
      "   Length: 1,572 characters (211 words)\n",
      "   Number of chunks: 3\n",
      "\n",
      "   First 200 characters:\n",
      "   \"The rise of Large Language Models (LLMs) in software engineering, particularly in code generation, has garnered significant attention. However, assessing the quality of AI-generated code remains a cha...\"\n",
      "\n",
      "   Last 200 characters:\n",
      "   \"... evaluating the quality of LLM-generated code remains a complex challenge due to the intricacies of programming concepts and syntax, which differ significantly from natural language generation [1, 2].\"\n",
      "\n",
      "üìö CONTEXT SOURCES:\n",
      "   Source 1 (similarity: 0.701)\n",
      "      Preview: The rise of Large Language Models (LLMs) in software engineering, particularly i...\n",
      "   Source 2 (similarity: 0.645)\n",
      "      Preview: The SBC score, along with the reverse-generated requirements, provides actionabl...\n",
      "   Source 3 (similarity: 0.628)\n",
      "      Preview: AI-powered code assistants, leveraging the power of Large Language Models (LLMs)...\n"
     ]
    }
   ],
   "source": [
    "# ===== CONTEXT ANALYSIS =====\n",
    "print(f\"\\nüß† CONTEXT PROVIDED TO LLM:\")\n",
    "print(f\"   Length: {len(context):,} characters ({len(context.split())} words)\")\n",
    "print(f\"   Number of chunks: {len(retrieved_lines_with_distances)}\")\n",
    "print(f\"\\n   First 200 characters:\")\n",
    "print(f\"   \\\"{context[:200]}...\\\"\")\n",
    "\n",
    "print(f\"\\n   Last 200 characters:\")\n",
    "print(f\"   \\\"...{context[-200:]}\\\"\")\n",
    "\n",
    "# Show the context sources\n",
    "print(f\"\\nüìö CONTEXT SOURCES:\")\n",
    "for i, (text, score) in enumerate(retrieved_lines_with_distances):\n",
    "    print(f\"   Source {i+1} (similarity: {score:.3f})\")\n",
    "    print(f\"      Preview: {text[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bba1be1-409b-48e4-b84d-b5c87face1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ RAG SYSTEM RESPONSE\n",
      "============================================================\n",
      "\n",
      "üí¨ GENERATED RESPONSE:\n",
      "The challenges of assessing the quality of AI-generated code include:\n",
      "\n",
      "1. Complexity of programming tasks\n",
      "2. Lack of robust evaluation metrics that align with human judgment\n",
      "3. Inherent differences between programming concepts and syntax and natural language generation\n",
      "\n",
      "Strategies for assessing AI-generated code quality include:\n",
      "\n",
      "1. Using the SBC score and reverse-generated requirements for actionable insights\n",
      "2. Addressing syntactic variations and alternative solutions in generated code\n",
      "3. Developing evaluation metrics that are specifically designed for code intelligence and verification tasks.\n",
      "\n",
      "üìä RESPONSE ANALYSIS:\n",
      "   ‚Ä¢ Response length: 602 characters\n",
      "   ‚Ä¢ Response words: 80\n",
      "   ‚Ä¢ Structure: Well-structured\n",
      "   ‚Ä¢ Addresses both challenges and strategies: Yes\n",
      "\n",
      "‚úÖ RAG PIPELINE COMPLETE!\n",
      "   The system successfully:\n",
      "   ‚Ä¢ Embedded the user's question\n",
      "   ‚Ä¢ Retrieved relevant document chunks\n",
      "   ‚Ä¢ Generated a contextually grounded response\n",
      "   ‚Ä¢ Provided specific, accurate information from the source document\n"
     ]
    }
   ],
   "source": [
    "# ===== FINAL RESPONSE ANALYSIS =====\n",
    "print(f\"\\nüéØ RAG SYSTEM RESPONSE\")\n",
    "print(f\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüí¨ GENERATED RESPONSE:\")\n",
    "print(f\"{ai_msg.content}\")\n",
    "\n",
    "print(f\"\\nüìä RESPONSE ANALYSIS:\")\n",
    "response_words = len(ai_msg.content.split())\n",
    "response_chars = len(ai_msg.content)\n",
    "print(f\"   ‚Ä¢ Response length: {response_chars} characters\")\n",
    "print(f\"   ‚Ä¢ Response words: {response_words}\")\n",
    "print(f\"   ‚Ä¢ Structure: {'Well-structured' if '1.' in ai_msg.content or '‚Ä¢' in ai_msg.content else 'Paragraph format'}\")\n",
    "print(f\"   ‚Ä¢ Addresses both challenges and strategies: {'Yes' if 'challenges' in ai_msg.content.lower() and 'strategies' in ai_msg.content.lower() else 'Partial'}\")\n",
    "\n",
    "print(f\"\\n‚úÖ RAG PIPELINE COMPLETE!\")\n",
    "print(f\"   The system successfully:\")\n",
    "print(f\"   ‚Ä¢ Embedded the user's question\")\n",
    "print(f\"   ‚Ä¢ Retrieved relevant document chunks\")\n",
    "print(f\"   ‚Ä¢ Generated a contextually grounded response\")\n",
    "print(f\"   ‚Ä¢ Provided specific, accurate information from the source document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67ec792-221a-45a8-af48-51deb40734b3",
   "metadata": {},
   "source": [
    "# üéì Conclusion: RAG System Complete!\n",
    "\n",
    "## What We Accomplished\n",
    "\n",
    "You've successfully built and run a complete RAG (Retrieval-Augmented Generation) system! Here's what we covered:\n",
    "\n",
    "### üìÑ **Document Ingestion**\n",
    "- Downloaded documents from object storage (MinIO/S3)\n",
    "- Processed PDF documents using advanced parsing (Docling)\n",
    "- Performed intelligent document chunking for optimal retrieval\n",
    "\n",
    "### üß† **Text Embeddings**\n",
    "- Learned about semantic vector representations\n",
    "- Used SentenceTransformers to generate 384-dimensional embeddings\n",
    "- Understood how embeddings capture semantic similarity\n",
    "\n",
    "### üóÉÔ∏è **Vector Database**\n",
    "- Set up Milvus for high-performance vector storage\n",
    "- Stored embeddings with metadata for efficient retrieval\n",
    "- Configured search parameters for optimal performance\n",
    "\n",
    "### üîç **Semantic Search**\n",
    "- Converted queries to embeddings for similarity search\n",
    "- Retrieved the most relevant document chunks\n",
    "- Analyzed similarity scores and retrieval quality\n",
    "\n",
    "### ü§ñ **Response Generation**\n",
    "- Designed effective prompts for contextual responses\n",
    "- Integrated with Llama 3.2 3B model for generation\n",
    "- Generated accurate, grounded responses using retrieved context\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### RAG Benefits\n",
    "- **Accuracy**: Responses grounded in specific document content\n",
    "- **Transparency**: See exactly which sources informed the answer\n",
    "- **Flexibility**: Easy to update knowledge by changing documents\n",
    "- **Efficiency**: No need to retrain models for new information\n",
    "\n",
    "### Technical Insights\n",
    "- **Embedding Quality**: Choice of embedding model impacts retrieval performance\n",
    "- **Chunking Strategy**: Proper document segmentation improves context relevance\n",
    "- **Prompt Engineering**: Well-designed prompts are crucial for quality responses\n",
    "- **Vector Search**: Semantic similarity enables meaning-based retrieval\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To extend this RAG system, consider:\n",
    "\n",
    "1. **Multiple Documents**: Expand to handle document collections\n",
    "2. **Advanced Chunking**: Implement hybrid or semantic chunking strategies\n",
    "3. **Reranking**: Add reranking models to improve retrieval quality\n",
    "4. **Evaluation Metrics**: Implement retrieval and generation quality metrics\n",
    "5. **Production Deployment**: Scale for production with distributed systems\n",
    "6. **Multi-modal RAG**: Extend to handle images, tables, and other content types\n",
    "\n",
    "## Learning Resources\n",
    "\n",
    "- **Vector Databases**: Explore other options like Pinecone, Weaviate, Chroma\n",
    "- **Embedding Models**: Try domain-specific or larger embedding models\n",
    "- **LLM Options**: Experiment with different language models and sizes\n",
    "- **Advanced RAG**: Learn about query expansion, hypothesis verification, and multi-hop reasoning\n",
    "\n",
    "**Congratulations!** You now understand the fundamentals of building production-ready RAG systems. This knowledge forms the foundation for many modern AI applications that combine retrieval and generation capabilities.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
